@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{Brown2020,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{Chiang2022,
    title = "Overcoming a Theoretical Limitation of Self-Attention",
    author = "Chiang, David  and
      Cholak, Peter",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.527/",
    doi = "10.18653/v1/2022.acl-long.527",
    pages = "7654--7664",
    abstract = "Although transformers are remarkably effective for many tasks, there are some surprisingly easy-looking regular languages that they struggle with. Hahn shows that for languages where acceptance depends on a single input symbol, a transformer`s classification decisions get closer and closer to random guessing (that is, a cross-entropy of 1) as input strings get longer and longer. We examine this limitation using two languages: PARITY, the language of bit strings with an odd number of 1s, and FIRST, the language of bit strings starting with a 1. We demonstrate three ways of overcoming the limitation implied by Hahn`s lemma. First, we settle an open question by constructing a transformer that recognizes PARITY with perfect accuracy, and similarly for FIRST. Second, we use layer normalization to bring the cross-entropy of both models arbitrarily close to zero. Third, when transformers need to focus on a single position, as for FIRST, we find that they can fail to generalize to longer strings; we offer a simple remedy to this problem that also improves length generalization in machine translation."
}

@incollection{10.7551/mitpress/4943.003.0128,
    author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
    isbn = {9780262267137},
    title = {(1986) D. E. Rumelhart, G. E. Hinton, and R. J. Williams, "Learning internal representations by error propagation," Parallel Distributed Processing: Explorations in the Microstructures of Cognition, Vol. I, D. E. Rumelhart and J. L. McClelland (Eds.) Cambridge, MA: MIT Press, pp. 318-362},
    booktitle = {Neurocomputing, Volume 1: Foundations of Research},
    publisher = {The MIT Press},
    year = {1988},
    month = {04},
    doi = {10.7551/mitpress/4943.003.0128},
    url = {https://doi.org/10.7551/mitpress/4943.003.0128},
    eprint = {https://direct.mit.edu/book/chapter-pdf/2299556/c018389\_9780262267137.pdf},
}

@inproceedings{Clark2020,
  title     = {Transformers as Soft Reasoners over Language},
  author    = {Clark, Peter and Tafjord, Oyvind and Richardson, Kyle},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI-20}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Christian Bessiere},
  pages     = {3882--3890},
  year      = {2020},
  month     = {7},
  note      = {Main track},
  doi       = {10.24963/ijcai.2020/537},
  url       = {https://doi.org/10.24963/ijcai.2020/537},
}

@inproceedings{Csordas2021,
    title = "The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers",
    author = "Csord{\'a}s, R{\'o}bert  and
      Irie, Kazuki  and
      Schmidhuber, Juergen",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.49",
    doi = "10.18653/v1/2021.emnlp-main.49",
    pages = "619--634",
    abstract = "Recently, many datasets have been proposed to test the systematic generalization ability of neural networks. The companion baseline Transformers, typically trained with default hyper-parameters from standard tasks, are shown to fail dramatically. Here we demonstrate that by revisiting model configurations as basic as scaling of embeddings, early stopping, relative positional embedding, and Universal Transformer variants, we can drastically improve the performance of Transformers on systematic generalization. We report improvements on five popular datasets: SCAN, CFQ, PCFG, COGS, and Mathematics dataset. Our models improve accuracy from 50{\%} to 85{\%} on the PCFG productivity split, and from 35{\%} to 81{\%} on COGS. On SCAN, relative positional embedding largely mitigates the EOS decision problem (Newman et al., 2020), yielding 100{\%} accuracy on the length split with a cutoff at 26. Importantly, performance differences between these models are typically invisible on the IID data split. This calls for proper generalization validation sets for developing neural networks that generalize systematically. We publicly release the code to reproduce our results.",
}
%"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers", Csordás 2021 % https://aclanthology.org/2021.emnlp-main.49/

@inproceedings{ontanon-etal-2022-making,
    title = "Making Transformers Solve Compositional Tasks",
    author = "Ontanon, Santiago  and
      Ainslie, Joshua  and
      Fisher, Zachary  and
      Cvicek, Vaclav",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.251/",
    doi = "10.18653/v1/2022.acl-long.251",
    pages = "3591--3607",
}

@inproceedings{Csordas2022,
title={The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization},
author={R{\'o}bert Csord{\'a}s and Kazuki Irie and J{\"u}rgen Schmidhuber},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=KBQP4A_J1K}
}

@inproceedings{Dehghani2019,
title={Universal Transformers},
author={Mostafa Dehghani and Stephan Gouws and Oriol Vinyals and Jakob Uszkoreit and Lukasz Kaiser},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HyzdRiR9Y7},
}

@article{FodorPylyshyn1988,
title = {Connectionism and cognitive architecture: A critical analysis},
journal = {Cognition},
volume = {28},
number = {1},
pages = {3-71},
year = {1988},
issn = {0010-0277},
doi = {https://doi.org/10.1016/0010-0277(88)90031-5},
url = {https://www.sciencedirect.com/science/article/pii/0010027788900315},
author = {Jerry A. Fodor and Zenon W. Pylyshyn},
abstract = {This paper explores differences between Connectionist proposals for cognitive architecture and the sorts of models that have traditionally been assumed in cognitive science. We claim that the major distinction is that, while both Connectionist and Classical architectures postulate representational mental states, the latter but not the former are committed to a symbol-level of representation, or to a ‘language of thought’: i.e., to representational states that have combinatorial syntactic and semantic structure. Several arguments for combinatorial structure in mental representations are then reviewed. These include arguments based on the ‘systematicity’ of mental representation: i.e., on the fact that cognitive capacities always exhibit certain symmetries, so that the ability to entertain a given thought implies the ability to entertain thoughts with semantically related contents. We claim that such arguments make a powerful case that mind/brain architecture is not Connectionist at the cognitive level. We then consider the possibility that Connectionism may provide an account of the neural (or ‘abstract neurological’) structures in which Classical cognitive architecture is implemented. We survey a number of the standard arguments that have been offered in favor of Connectionism, and conclude that they are coherent only on this interpretation.
Résumé
Cet articleétudie les différences entre modèles connectionistes et modèles classiques de la structure cognitive. Nous pensons que, bien que les deux types de modèles stipulent l'existence d'états mentaux représentationnels, la différence essentielle est que seuls les modèles classiques requièrent l'existence d'un niveau de représentation symbolique—un “langage de la pensée”—, c'est-à-dire d'états représentationnels possédant une structure syntaxique et sémantique. Nous examinons ensuite différents arguments qui militent en faveur de l'existence de représentations mentales ayant ces propriétés. Certains de ces arguments reposent sur la “systématicité” des représentations mentales, c'est-à-dire sur le fait que les capacités cognitives exhibent toujours certaines symétries, de sorte que la capacitéd'entretenir certaines pensées implique la capacitéd'entretenir d'autres pensées apparentées par leur contenu sémantique. Nous pensons que ces arguments montrent de manière convainquante que l'architecture de l'esprit/du cerveau n'est pas connectioniste au niveau cognitif. Nous nous demandons ensuite s'il est possible d'interpréter le connectionisme comme une analyse des structures neuronales (ou des structures neurologiques “abstraites”) dans lesquelles est réalisée l'architecture cognitive classique. Nous examinons plusieurs des arguments avancés habituellement en défense du connectionisme, et en concluons que ceux-ci n'ont de sens que dans cette interprétation.}
}

@misc{Friedman2023,
      title={Learning Transformer Programs}, 
      author={Dan Friedman and Alexander Wettig and Danqi Chen},
      year={2023},
      eprint={2306.01128},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.01128}, 
}

@inproceedings{KimLinzen2020,
    title = "{COGS}: A Compositional Generalization Challenge Based on Semantic Interpretation",
    author = "Kim, Najoung  and
      Linzen, Tal",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.731",
    doi = "10.18653/v1/2020.emnlp-main.731",
    pages = "9087--9105",
    abstract = "Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96{--}99{\%}), but generalization accuracy was substantially lower (16{--}35{\%}) and showed high sensitivity to random seed (+-6{--}8{\%}). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.",
}

@inproceedings{Ranzato2015,
  author       = {Marc'Aurelio Ranzato and
                  Sumit Chopra and
                  Michael Auli and
                  Wojciech Zaremba},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Sequence Level Training with Recurrent Neural Networks},
  booktitle    = {4th International Conference on Learning Representations, {ICLR} 2016,
                  San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year         = {2016},
  url          = {http://arxiv.org/abs/1511.06732},
  timestamp    = {Thu, 25 Jul 2019 14:25:39 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/RanzatoCAZ15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.1145/1553374.1553380,
author = {Bengio, Yoshua and Louradour, J\'{e}r\^{o}me and Collobert, Ronan and Weston, Jason},
title = {Curriculum learning},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553380},
doi = {10.1145/1553374.1553380},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {41–48},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@inproceedings{Reddy2017,
    title = "Universal Semantic Parsing",
    author = {Reddy, Siva  and
      T{\"a}ckstr{\"o}m, Oscar  and
      Petrov, Slav  and
      Steedman, Mark  and
      Lapata, Mirella},
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1009",
    doi = "10.18653/v1/D17-1009",
    pages = "89--101",
    abstract = "Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that semantic parsing can be accomplished by transforming syntactic dependencies to logical forms. However, this work is limited to English, and cannot process dependency graphs, which allow handling complex phenomena such as control. In this work, we introduce UDepLambda, a semantic interface for UD, which maps natural language to logical forms in an almost language-independent fashion and can process dependency graphs. We perform experiments on question answering against Freebase and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation. Results show that UDepLambda outperforms strong baselines across languages and datasets. For English, it achieves a 4.9 F1 point improvement over the state-of-the-art on GraphQuestions.",
}

@article{Strobl2024,
   title={What Formal Languages Can Transformers Express? A Survey},
   volume={12},
   ISSN={2307-387X},
   url={http://dx.doi.org/10.1162/tacl_a_00663},
   DOI={10.1162/tacl_a_00663},
   journal={Transactions of the Association for Computational Linguistics},
   publisher={MIT Press},
   author={Strobl, Lena and Merrill, William and Weiss, Gail and Chiang, David and Angluin, Dana},
   year={2024},
   pages={543–561} }

@InProceedings{Weiss2021,
  title = 	 {Thinking Like Transformers},
  author =       {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11080--11090},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/weiss21a/weiss21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/weiss21a.html},
  abstract = 	 {What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder—attention and feed-forward computation—into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.}
}

@misc{Zaremba2016,
      title={Reinforcement Learning Neural Turing Machines - Revised}, 
      author={Wojciech Zaremba and Ilya Sutskever},
      year={2016},
      eprint={1505.00521},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1505.00521}, 
}

@article{Wu2023,
    title = "{R}e{COGS}: How Incidental Details of a Logical Form Overshadow an Evaluation of Semantic Interpretation",
    author = "Wu, Zhengxuan  and
      Manning, Christopher D.  and
      Potts, Christopher",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2023",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2023.tacl-1.96/",
    doi = "10.1162/tacl_a_00623",
    pages = "1719--1733",
    abstract = "Compositional generalization benchmarks for semantic parsing seek to assess whether models can accurately compute meanings for novel sentences, but operationalize this in terms of logical form (LF) prediction. This raises the concern that semantically irrelevant details of the chosen LFs could shape model performance. We argue that this concern is realized for the COGS benchmark (Kim and Linzen, 2020). COGS poses generalization splits that appear impossible for present-day models, which could be taken as an indictment of those models. However, we show that the negative results trace to incidental features of COGS LFs. Converting these LFs to semantically equivalent ones and factoring out capabilities unrelated to semantic interpretation, we find that even baseline models get traction. A recent variable-free translation of COGS LFs suggests similar conclusions, but we observe this format is not semantically equivalent; it is incapable of accurately representing some COGS meanings. These findings inform our proposal for ReCOGS, a modified version of COGS that comes closer to assessing the target semantic capabilities while remaining very challenging. Overall, our results reaffirm the importance of compositional generalization and careful benchmark task design."
}

@inproceedings{Zhou2024,
title={What Algorithms can Transformers Learn? A Study in Length Generalization},
author={Hattie Zhou and Arwen Bradley and Etai Littwin and Noam Razin and Omid Saremi and Joshua M. Susskind and Samy Bengio and Preetum Nakkiran},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=AssIuHnmHX}
}

@article{newell1956logic,
  title={The logic theory machine--A complex information processing system},
  author={Newell, Allen and Simon, Herbert},
  journal={IRE Transactions on information theory},
  volume={2},
  number={3},
  pages={61--79},
  year={1956},
  publisher={IEEE}
}

@misc{mccarthy1959programs,
  title={Programs with common sense},
  author={McCarthy, John},
  year={1959},
  publisher={London}
}

@inproceedings{delétang2023neuralnetworkschomskyhierarchy,
title={Neural Networks and the Chomsky Hierarchy},
author={Gregoire Deletang and Anian Ruoss and Jordi Grau-Moya and Tim Genewein and Li Kevin Wenliang and Elliot Catt and Chris Cundy and Marcus Hutter and Shane Legg and Joel Veness and Pedro A Ortega},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=WbxHAzkeQcn}
}

@article{Perez2021,
  author  = {Jorge Pérez and Pablo Barceló and Javier Marinkovic},
  title   = {Attention is Turing-Complete},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {75},
  pages   = {1--35},
  url     = {http://jmlr.org/papers/v22/20-302.html}
}

@inproceedings{merrill2024expressivepowertransformerschain,
title={The Expressive Power of Transformers with Chain of Thought},
author={William Merrill and Ashish Sabharwal},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=NjNGlPh8Wh}
}

@inproceedings{lindner2023tracrcompiledtransformerslaboratory,
title={Tracr: Compiled Transformers as a Laboratory for Interpretability},
author={David Lindner and Janos Kramar and Sebastian Farquhar and Matthew Rahtz and Thomas McGrath and Vladimir Mikulik},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=tbbId8u7nP}
}

@incollection{fuzzingbook2023:GrammarCoverageFuzzer,
    author = {Andreas Zeller and Rahul Gopinath and Marcel B{\"o}hme and Gordon Fraser and Christian Holler},
    booktitle = {The Fuzzing Book},
    title = {Grammar Coverage},
    year = {2023},
    publisher = {CISPA Helmholtz Center for Information Security},
    howpublished = {\url{https://www.fuzzingbook.org/html/GrammarCoverageFuzzer.html}},
    note = {Retrieved 2023-11-11 18:18:06+01:00},
    url = {https://www.fuzzingbook.org/html/GrammarCoverageFuzzer.html},
    urldate = {2023-11-11 18:18:06+01:00}
}

@misc{klinger2024compositionalprogramgenerationfewshot,
      title={Compositional Program Generation for Few-Shot Systematic Generalization}, 
      author={Tim Klinger and Luke Liu and Soham Dan and Maxwell Crouse and Parikshit Ram and Alexander Gray},
      year={2024},
      eprint={2309.16467},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.16467}, 
}

@inproceedings{tenney2019bertrediscoversclassicalnlp,
    title = "{BERT} Rediscovers the Classical {NLP} Pipeline",
    author = "Tenney, Ian  and
      Das, Dipanjan  and
      Pavlick, Ellie",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1452/",
    doi = "10.18653/v1/P19-1452",
    pages = "4593--4601",
    abstract = "Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations."
}

@article{lake2023human,
  title={Human-like systematic generalization through a meta-learning neural network},
  author={Lake, Brenden M and Baroni, Marco},
  journal={Nature},
  volume={623},
  number={7985},
  pages={115--121},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{linzen2016assessing,
  title={Assessing the ability of LSTMs to learn syntax-sensitive dependencies},
  author={Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
  journal={Transactions of the Association for Computational Linguistics},
  volume={4},
  pages={521--535},
  year={2016},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@book{jespersen1913modernenglishgrammar1954reprint,
  title={A Modern English Grammar on Historical Principles: Volume 2, Syntax (first volume)},
  author={Jespersen, O.},
  series={Otto Jespersen},
  year={1954},
  publisher={Bradford and Dickens}
}


@misc{agreementwithnearestlanguagelog,
      title={Agreement with nearest}, 
      author={Arnold Zwicky},
      year={2008},
      url={https://languagelog.ldc.upenn.edu/nll/?p=839}, 
}

@inproceedings{petty2024impactdepthcompositionalgeneralization,
    title = "The Impact of Depth on Compositional Generalization in Transformer Language Models",
    author = "Petty, Jackson  and
      Steenkiste, Sjoerd  and
      Dasgupta, Ishita  and
      Sha, Fei  and
      Garrette, Dan  and
      Linzen, Tal",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.402/",
    doi = "10.18653/v1/2024.naacl-long.402",
    pages = "7239--7252",
    abstract = "To process novel sentences, language models (LMs) must generalize compositionally{---}combine familiar elements in new ways. What aspects of a model`s structure promote compositional generalization? Focusing on transformers, we test the hypothesis, motivated by theoretical and empirical work, that deeper transformers generalize more compositionally. Simply adding layers increases the total number of parameters; to address this confound between depth and size, we construct three classes of models which trade off depth for width such that the total number of parameters is kept constant (41M, 134M and 374M parameters). We pretrain all models as LMs and fine-tune them on tasks that test for compositional generalization. We report three main conclusions: (1) after fine-tuning, deeper models generalize more compositionally than shallower models do, but the benefit of additional layers diminishes rapidly; (2) within each family, deeper models show better language modeling performance, but returns are similarly diminishing; (3) the benefits of depth for compositional generalization cannot be attributed solely to better performance on language modeling. Because model latency is approximately linear in the number of layers, these results lead us to the recommendation that, with a given total parameter budget, transformers can be made shallower than is typical without sacrificing performance."
}

@inproceedings{vanschijndel2019quantitydoesntbuyquality,
    title = "Quantity doesn`t buy quality syntax with neural language models",
    author = "van Schijndel, Marten  and
      Mueller, Aaron  and
      Linzen, Tal",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1592/",
    doi = "10.18653/v1/D19-1592",
    pages = "5831--5837",
    abstract = "Recurrent neural networks can learn to predict upcoming words remarkably well on average; in syntactically complex contexts, however, they often assign unexpectedly high probabilities to ungrammatical words. We investigate to what extent these shortcomings can be mitigated by increasing the size of the network and the corpus on which it is trained. We find that gains from increasing network size are minimal beyond a certain point. Likewise, expanding the training corpus yields diminishing returns; we estimate that the training corpus would need to be unrealistically large for the models to match human performance. A comparison to GPT and BERT, Transformer-based models trained on billions of words, reveals that these models perform even more poorly than our LSTMs in some constructions. Our results make the case for more data efficient architectures."
}

@misc{goldberg2019assessingbertssyntacticabilities,
      title={Assessing BERT's Syntactic Abilities}, 
      author={Yoav Goldberg},
      year={2019},
      eprint={1901.05287},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1901.05287}, 
}

@inproceedings{li2023slogstructuralgeneralizationbenchmark,
    title = "{SLOG}: A Structural Generalization Benchmark for Semantic Parsing",
    author = "Li, Bingzhi  and
      Donatelli, Lucia  and
      Koller, Alexander  and
      Linzen, Tal  and
      Yao, Yuekun  and
      Kim, Najoung",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.194/",
    doi = "10.18653/v1/2023.emnlp-main.194",
    pages = "3213--3232",
    abstract = "The goal of compositional generalization benchmarks is to evaluate how well models generalize to new complex linguistic expressions. Existing benchmarks often focus on lexical generalization, the interpretation of novel lexical items in syntactic structures familiar from training; structural generalization tasks, where a model needs to interpret syntactic structures that are themselves unfamiliar from training, are often underrepresented, resulting in overly optimistic perceptions of how well models can generalize. We introduce SLOG, a semantic parsing dataset that extends COGS (Kim and Linzen, 2020) with 17 structural generalization cases. In our experiments, the generalization accuracy of Transformer models, including pretrained ones, only reaches 40.6{\%}, while a structure-aware parser only achieves 70.8{\%}. These results are far from the near-perfect accuracy existing models achieve on COGS, demonstrating the role of SLOG in foregrounding the large discrepancy between models' lexical and structural generalization capacities."
}

@inproceedings{hewitt-manning-2019-structural,
    title = "{A} Structural Probe for Finding Syntax in Word Representations",
    author = "Hewitt, John  and
      Manning, Christopher D.",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1419/",
    doi = "10.18653/v1/N19-1419",
    pages = "4129--4138",
    abstract = "Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network`s word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models' vector geometry."
}

@article{FRANCK2006173,
title = {Agreement and movement: A syntactic analysis of attraction},
journal = {Cognition},
volume = {101},
number = {1},
pages = {173-216},
year = {2006},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2005.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0010027705001861},
author = {Julie Franck and Glenda Lassi and Ulrich H. Frauenfelder and Luigi Rizzi},
keywords = {Sentence production, Subject–verb agreement, Attraction, Movement, Experimental psycholinguistics, Formal syntax},
abstract = {This paper links experimental psycholinguistics and theoretical syntax in the study of subject–verb agreement. Three experiments of elicited spoken production making use of specific characteristics of Italian and French are presented. They manipulate and examine its impact on the occurrence of ‘attraction’ errors (i.e. incorrect agreement with a word that is not the subject of the sentence). Experiment 1 (in Italian) shows that subject modifiers do not trigger attraction errors in free inverted VS (Verb Subject) structures, although attraction was found in VS interrogatives in English (Vigliocco, G., & Nicol, J. (1998). Separating hierarchical relations and word order in language production. Is proximity concord syntactic or linear? Cognition, 13–29) In Experiment 2 (in French), we report stronger attraction with preverbal clitic object pronouns than with subject modifiers. Experiment 3 (in French) shows that displaced direct objects in the cleft construction trigger attraction effects, in spite of the fact that the object does not intervene between the subject and the verb in the surface word order (OSV). Moreover, attraction is stronger in structures with subject–verb inversion (...). These observations are shown to be naturally interpretable through the tools of formal syntax, as elaborated within the Principles and Parameters/Minimalist tradition. Three important constructs are discussed: (1) the hierarchical representation of the sentence during syntactic construction, and the role of intermediate positions by which words transit when they move; (2) the role of specific hierarchical (c-command) but also linear (precedence) relations; and (3) the possibility that agreement involves two functionally distinct components. A gradient of computational complexity in agreement is presented which relates empirical evidence to these theoretical constructs.}
}

@article{VIGLIOCCO1998B13,
title = {Separating hierarchical relations and word order in language production: is proximity concord syntactic or linear?},
journal = {Cognition},
volume = {68},
number = {1},
pages = {B13-B29},
year = {1998},
issn = {0010-0277},
doi = {https://doi.org/10.1016/S0010-0277(98)00041-9},
url = {https://www.sciencedirect.com/science/article/pii/S0010027798000419},
author = {Gabriella Vigliocco and Janet Nicol},
keywords = {Agreement errors, Grammatical encoding, Language production}
}

@misc{petty2021transformersgeneralizelinearly,
      title={Transformers Generalize Linearly}, 
      author={Jackson Petty and Robert Frank},
      year={2021},
      eprint={2109.12036},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2109.12036}, 
}

@inproceedings{murty2023grokkinghierarchicalstructurevanilla,
    title = "Grokking of Hierarchical Structure in Vanilla Transformers",
    author = "Murty, Shikhar  and
      Sharma, Pratyusha  and
      Andreas, Jacob  and
      Manning, Christopher",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.38/",
    doi = "10.18653/v1/2023.acl-short.38",
    pages = "439--448",
    abstract = "For humans, language production and comprehension is sensitive to the hierarchical structure of sentences. In natural language processing, past work has questioned how effectively neural sequence models like transformers capture this hierarchical structure when generalizing to structurally novel inputs. We show that transformer language models can learn to generalize hierarchically after training for extremely long periods{---}far beyond the point when in-domain accuracy has saturated. We call this phenomenon structural grokking. On multiple datasets, structural grokking exhibits inverted U-shaped scaling in model depth: intermediate-depth models generalize better than both very deep and very shallow transformers. When analyzing the relationship between model-internal properties and grokking, we find that optimal depth for grokking can be identified using the tree-structuredness metric of CITATION. Overall, our work provides strong evidence that, with extended training, vanilla transformers discover and use hierarchical structure."
}

@inproceedings{murty2022characterizingintrinsiccompositionalitytransformers,
title={Characterizing intrinsic compositionality in transformers with Tree Projections},
author={Shikhar Murty and Pratyusha Sharma and Jacob Andreas and Christopher D Manning},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=sAOOeI878Ns}
}

@article{10.1162/tacl_a_00733,
    author = {Ahuja, Kabir and Balachandran, Vidhisha and Panwar, Madhur and He, Tianxing and Smith, Noah A. and Goyal, Navin and Tsvetkov, Yulia},
    title = {Learning Syntax Without Planting Trees: Understanding Hierarchical Generalization in Transformers},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {13},
    pages = {121-141},
    year = {2024},
    month = {02},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00733},
    url = {https://doi.org/10.1162/tacl\_a\_00733},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00733/2501791/tacl\_a\_00733.pdf},
}

@inproceedings{pennington-etal-2014-glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162/",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543"
}