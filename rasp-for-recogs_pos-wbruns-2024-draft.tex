% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{acl}
%\usepackage[review]{acl} % note this has nothing to do with access control, this is association for computational linguistics paper formatting
% there is no sensitive data in this paper whatsoever

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{(DRAFT) Exploring Compositional Generalization (in ReCOGS\_pos) by Transformers using Restricted Access Sequence Processing (RASP)}

\author{William Bruns \\
  \texttt{adde.animulis@gmail.com}
}

\begin{document}
\maketitle
\begin{abstract}
Humans understand new combinations of words encountered if they are combinations of words recognized from different contexts, an ability called Compositional Generalization.
%Some observations contradict that Transformers learn systematic, compositional solutions to problems that generalize. 
The COGS benchmark \cite{KimLinzen2020} reports 0\% accuracy for Transformer models on some structural generalizations.
We use \cite{Weiss2021}'s Restricted Access Sequence Processing (RASP), a Transformer-equivalent programming language, to prove by construction that a Transformer encoder-decoder
%\footnote{We follow \cite{Zhou2024} who used RASP to analyze encoder-decoder and decoder-loop cases, not just Transformer encoders as done by RASP author \cite{Weiss2021}.} 
can perform the semantically equivalent ReCOGS\_pos
%\footnote{ReCOGS\_pos is closer to COGS than ReCOGS: COGS and ReCOGS\_pos are both positional, non-positional ReCOGS randomizes indices and makes measuring string exact match impossible, only semantic exact match. Since our RASP model can produce string exact match results in most cases and we are working on a RASP model for COGS, we stuck with ReCOGS\_pos. \cite{Wu2023} hypothesized randomizing the indices would improve generalization, but that was actually not the case.} 
\cite{Wu2023} variant of COGS systematically and compositionally: Our RASP model attains 100\% semantic exact match\footnote{\begin{footnotesize}and 100\% string exact match\end{footnotesize}} on the ReCOGS test set and 100\% SEM on all generalization splits except obj\_pp\_to\_subj\_pp\footnote{\begin{footnotesize}where, in the generalization case, prepositional nouns are frequently inserted between the leading noun and the verb it is related to, unlike the training case for those verb types, and models (our RASP model, without a specific rule to avoid attraction errors learnable from training data, and we show also the baseline Transformer trained from scratch) in this case tend to an attraction error to a nearer noun, now a prepositional phrase noun. \textit{See "Appendix: Attraction errors"} (\ref{attraction_errors})\end{footnotesize}} which gets 92\%. Furthermore, our RASP model shows the ReCOGS\_pos task does not require a hierarchical or tree-structured solution: we use word-level tokens with an "embedding" layer that tags with possible part of speech\footnote{\begin{footnotesize}per \cite{tenney2019bertrediscoversclassicalnlp} by layer 0 the part-of-speech can be predicted for most words in real pre-trained deep Transformers already, so we assume that is learnable\end{footnotesize}}, applying just once per encoder pass 19 attention-head compatible flat pattern-matching rules, shown using grammar coverage \cite{fuzzingbook2023:GrammarCoverageFuzzer} to be learnable from the training data, plus general prepositional phrase (pp) handling and complement phrase (cp) handling logic, and output the next logical form (LF) token (repeating until the LF is complete). The model does not apply recursive, tree-structured rules like `np\_det pp np -> np\_pp -> np`, but scores 100\% semantic and string exact match on pp recursion, cp recursion using the decoder loop.
%We find a single flat pattern matching rule we originally added to fit training examples, to ignore "pp det common\_noun" and "pp proper\_noun" when matching nouns for the agent, theme, recipient right indices, is sufficient to avoid structural generalization errors due to pp modification in novel positions. Interestingly, we imagined ablating that single rule and hypothesized attraction to the nearest noun\footnote{\begin{footnotesize}
%Specifically, a non-agent noun which was part of the actual attention-head compatible verb-centered pattern match (closer to the verb than the actual agent) in our RASP model getting labeled the agent. See "Appendix: Attraction errors" (\ref{attraction_errors}) for detailed definition, examples, and justification for the term/historical references.
%\end{footnotesize}
%} in its absence and found this predicted the exact error 
%(the nearest noun to the verb is mistaken for the agent of the verb) 
%in 96\% of the single relationship errors the \cite{Wu2023} baseline Transformers make on the obj-pp-to-subj-pp split\footnote{overall, their semantic exact match on the split is measured by us at 19.7\%, consistent with their Figure 5} when the agent is left of the verb in single verb sentences\footnote{\begin{footnotesize}
%we also predict a new split "v\_dat\_p2\_pp\_moved\_to\_recipient" based on "attraction" errors expected of the flat non-hierarchical, non-tree model and experimentally confirm it is just as difficult for the baseline Transformer
%\end{footnotesize}} (suggesting perhaps that the baseline \cite{Wu2023} Transformer trained from scratch is also not learning a hierarchical, tree-structured representation.) 
%Thus we demonstrate using RASP that hierarchical/tree-like solutions
%(beyond mapping words to parts-of-speech)
%are not required for high accuracy on ReCOGS which is semantically equivalent to COGS and argue it is likely Transformer models will be able to perform the ReCOGS\_pos near perfectly including the structural generalization splits, but a different learning strategy may be required.
\end{abstract}

\section{Introduction}
It was long argued that connectionist models (i.e. neural networks) were somehow structurally incapable of compositional learning \cite{FodorPylyshyn1988}.\footnote{\begin{footnotesize}
This debate does continue in particular areas, for example just in the domain of syntax, one can read \cite{vanschijndel2019quantitydoesntbuyquality} vs \cite{goldberg2019assessingbertssyntacticabilities}.
\end{footnotesize}
} 
However, large language models based on the Transformers architecture compose seemingly fluent and novel text and are excellent few or zero shot learners \cite{Brown2020}.

Some observations do contradict that Transformers learn systematic, compositional solutions to problems that generalize\footnote{\begin{footnotesize}See "Appendix: Composition and Learning" (\ref{composition_and_learning})\end{footnotesize}}, for example structural generalizations
%\footnote{\begin{footnotesize}especially prepositional phrase generalizations\end{footnotesize}} 
in the COGS task and ReCOGS\cite{Wu2023} variant of the COGS task \cite{KimLinzen2020}
%\footnote{
%\begin{footnotesize}
%even with \cite{lake2023human}'s "meta-learning for compositionality" framework for Transformers described as achieving "human-like systematic generalization",their error rate on the COGS structural generalizations was still 100\%
%\end{footnotesize}
%}
, a benchmark based on extracting semantics (logical form) from the syntax (grammatical form) of synthetic sentences in a simplified subset of English grammar, requiring models trained only on certain grammar examples to generalize to sentences with unseen grammar built up / recombined from parts present in the training examples.


We use \cite{Weiss2021}'s Restricted Access Sequence Processing (RASP) language that can be compiled to concrete Transformer weights to prove by construction that a Transformer encoder-decoder\footnote{\begin{footnotesize}We follow \cite{Zhou2024} who used RASP to analyze encoder-decoder and decoder-loop cases, not just Transformer encoders as done by RASP author \cite{Weiss2021}.\end{footnotesize}} can perform ReCOGS\_pos\footnote{\begin{footnotesize}official variant, closer to COGS than non-positional ReCOGS, as COGS is also positional, and means we can also measure string exact match, not just semantic exact match\end{footnotesize}} over the vocabulary and grammar of that task in a systematic, compositional way (length and recursion depth limited) as a rigorous starting point to investigating when Transformers might learn or not actually learn such compositional/systematic solutions. We find a flat, not hierarchical/tree-structured model which lacks any handling for the recursive rules in the grammar (for prepositional phrase recursion and complement phrase recursion) can perform the task at high accuracy, but requires a special rule for avoiding "attraction" errors\footnote{\begin{footnotesize}These attraction errors are similar to those discussed elsewhere in NLP and psycholinguistics literature on hierarchical vs linear processing by language models and humans, see "Appendix: Attraction errors" (\ref{attraction_errors}).\end{footnotesize}} where inserted prepositional phrase nouns replace agent/theme/recipient nouns in the logical form by accident. This is our main result and suggests that the ReCOGS task can be performed with high accuracy by Transformers, turning efforts to learnability, and also adds to the literature a caveat on interpreting success on ReCOGS by noting it does not necessarily require a hierarchical or tree-based approach. Finally, we predict that these "attraction" errors we had to specifically avoid in our RASP model are contributing to the high error rate of the \cite{Wu2023} baseline Transformer trained from scratch and confirm this is the case.

%As a secondary result, we also find imagining ablating the RASP model rule for avoiding "attraction" errors and replacing it with attraction to the nearest noun allows us to (1) predict 96\% of a category of obj-pp-to-subj-pp errors made by the \cite{Wu2023} baseline Encoder-Decoder Transformer (the exact mismatch in the agent index), and (2) predict a new generalization split which does not involve subjects, objects, or pp-modifying a noun of the left side of a verb, but is predicted to have "attraction" errors, and is confirmed to be as hard as the current hardest reported "obj-pp-to-subj-pp" split.

%However, the RASP model also leads to a hypothesis that the \cite{Wu2023} baseline Encoder-Decoder Transformers are perhaps also generalizing poorly on the "hardest split" obj-pp-to-subj-pp due to not learning a hierarchical, tree-structured representation capable of learning the rule `np\_det pp np -> np`. We imagine ablating the single RASP model rule for ignoring prepositional phrase nouns inserted between agent/theme nouns and their related verb and replacing it with attraction to the nearest noun and predict the details (see results and discussion) of 96\% of the baseline Encoder-Decoder single-verb sentence single-part errors when the agent is left of the verb in the obj-pp-to-subj-pp split (where the subject is the agent for simplicity), even holding true for all cases where two attractor prepositional phrase nouns are present (the nearest to the verb becomes the agent by mistake).
%This guides us to try testing the model on a very different non-subject vs object syntax also predicted to have "attraction" errors, modifying recipient nouns on the right side of verb in the v\_dat\_p2 form (despite being on the right side, the "(v\_dat\_p2 verb) (recipient noun) (theme noun)" form means this still places a distractor in between the verb and the theme noun for that verb), which the baseline model performs similarly poorly on as predicted by the RASP model. (However, trying to fix this by augmenting the data with these `np v\_dat\_p2 np\_det pp np np` examples does not yet show any crossover benefit to both splits.) Finally, inspired by \cite{Csordas2022}'s hypothesis about the number of layers required to compositionally model a parsing problem in terms of the depth of the parse tree (compositional operations at each level suggests depth must exceed the height of the parse tree), we check whether adding layers improves performance on the obj-pp-to-subj-pp split. We find no performance benefit to 3 or 4 layers instead of 2 on structural generalization, consistent with \cite{petty2024impactdepthcompositionalgeneralization} who explored up to 32 layers for COGS, further suggesting that a hierarchical or tree-structured, recursive solution is NOT learned by the \cite{Wu2023} baseline Encoder-Decoder Transformer for this simple grammar (our RASP model indicates a hierarchical, tree-structured solution is NOT needed for high accuracy and if the rule to ignore attractor/distractor nouns is not learned, predicts the aforementioned observed errors from a non-tree structured model). This explains the challenge of generalizing on unseen prepositional phrase related modification related splits as arising from the baseline 2 to 4 layer Encoder-Decoder Transformers not being able to leverage the recursive/tree-structured grammar rule `np\_det pp np -> np\_pp -> np` during learning and which requires them to instead actually observe more of the various prepositional phrase substitutions to learn them.

\begin{figure}
\includegraphics[scale=0.560]{./image_pdfs/expanded_version_of_sentences_and_lfs_and_lf_graph_figure.pdf}
\caption{\begin{small}Introducing parse trees, logical form, and semantic graphs. Two semantically identical but syntactically distinct (i) sentences (a) "a boy painted the girl" and (b) "the girl was painted by a boy" are shown with (ii) their distinct parse tree (parsed into COGS input grammar), (iii) the string form of their semantics (ReCOGS logical form; differs in indices and ordering), and (c) the graph representation of their logical form (semantic graph\protect\footnotemark, not different at all between the two examples). Note the (iii) logical forms (LFs) differ by String Exact Match but not \cite{Wu2023}'s Semantic Exact Match (order and indices do not match but nouns, normalized verbs, and relationships between nouns and verbs are same). Note the "agent", "theme" order in the logical form string is not required to match for Semantic Exact Match. COGS and ReCOGS tasks require extracting the semantics/meaning (c) encoded in LFs (iii) of sentences (i).\end{small}}
\label{data-and-task-figure}
\end{figure}

\footnotetext{\begin{footnotesize}As a convention, in converting ReCOGS logical forms to Semantic Graphs we use the logical form (source, target) index order for directed semantic graph edges (from verb to related entity) EXCEPT for the agent relationship which is from the agent of a verb to the verb (opposite direction from logical form in that case), which gives our semantic graphs of ReCOGS sentences an unambiguous starting point (layout starts from agent) without affecting comparison of the graphs (generated by a consistent rule), see also Figure \ref{baseline_transformer_standard_training_cannot_do_v_dat_p2_generalization}.\end{footnotesize}}
\section{Prior Literature}

\cite{KimLinzen2020} introduce the COmpositional Generalization Challenge based on Semantic Interpretation (COGS) benchmark\footnote{
\begin{footnotesize}
See Figure \ref{data-and-task-figure}.
\end{footnotesize}
} and argue that Transformers have low accuracy on the generalization splits (35\% overall), especially structural generalization splits where near 0\% accuracy is reported, using a 2-layer Encoder-Decoder Transformer (2 layers for Encoder, 2 layers for Decoder).

For another example, \cite{lake2023human} use a "meta-learning for compositionality" approach with a 3-layer Encoder-Decoder Transformer architecture and achieve what they call "human-like systematic generalization", achieving high scores on everything in the COGS benchmark (>99\% on lexical generalizations) EXCEPT the structural generalization splits where they also still score 0\% accuracy.
However, one notices these networks are shallow compared with those used in successful large-pretrained Transformer models (e.g. 24-layer BERT where compositional parse trees seem to be encoded in its vector space representation \cite{hewitt-manning-2019-structural}), and it is claimed, by e.g. \cite{Csordas2022} that for compositional operations, like parsing, the depth of the network must be at least the maximum number of compositional operations, e.g. the height of the parse tree for grammar dependent problems. Remarkably, \cite{petty2024impactdepthcompositionalgeneralization} further claims that increasing the layer depth of the Transformer models (up to 32 layers) does not improve the near 0\% accuracy on COGS structural generalization splits like prepositional phrase modification of subject when the network has only seen it on the object during training and also input length/depth generalizations (like pp/cp recursion), perhaps surprising as for the simpler logical inferences problem in \cite{Clark2020} they observed successful logical inference depth generalization even by Encoder-only Transformers.

Thankfully, \cite{Wu2023} are able to begin to get traction (low but nonzero accuracy) for the shallow Encoder-Decoder Transformer models on structural generalizations in a modified but semantically equivalent form of the COGS task they call ReCOGS, which we analyze here. They remove redundant symbols, and use Semantic Exact Match instead of Exact Match (see Figure \ref{data-and-task-figure}).

%Ideally we would like a framework which can take our task at hand and analyze it to predict whether a Transformer in general will do well at it (without being sensitive to any particular data examples we have on hand), with some course of action provided for how a difficult task may be modified to make it more amenable to a Transformer, with the framework also telling us how many attention heads and layers will be needed for tasks which are learnable by Transformers.

\cite{Zhou2024} apply \cite{Weiss2021}'s RASP language to explain some inconsistent findings regarding generalization and use RASP to predict exactly which cases of generalization come easily to Transformers and which do not. \cite{Zhou2024} seem to reveal \cite{Weiss2021} has provided the framework we seek by demonstrating how to apply RASP to Transformer decoders with intermediate steps, and even use it to learn how to modify difficult-to-learn tasks like Parity\footnote{\begin{footnotesize}
See \cite{Strobl2024} for context from formal language theory, computational complexity, circuit complexity theory, and experimental papers together, providing robust lower and upper bounds on what Transformers can do, including discussion of under what conditions Parity can be solved by Transformers and how whether it can be learned by randomly initialized Transformers under simple training schemes is a different question (general feed-forward neural networks can learn to solve Parity per \cite{10.7551/mitpress/4943.003.0128}). \cite{delétang2023neuralnetworkschomskyhierarchy} also.
\end{footnotesize}} and long addition in seemingly incidental ways based on RASP analysis to make them readily learnable by Transformers in a compositional, length generalizing way!\footnote{See "Appendix: Zhou et al 2024 relevance of their long addition experiment to language modeling and note on the Parity task and Transformers" (\ref{appendix_long_addition_and_parity})}

Thus we apply techniques similar to \cite{Zhou2024} and \cite{Weiss2021} to ReCOGS to (1) argue Transformers should be capable of performing the task, including the structural generalization splits, with high accuracy, and that the problem is learning not capability and (2) try to assess how \cite{Wu2023}'s baseline Encoder-Decoder Transformers learn to extract semantics (logical form) from the syntax (grammatical form) of sentences and understand the prepositional phrase modification related generalization errors they are making.

%\section{Prior Literature}
%
%In "Transformers as Soft Reasoners over Language" \cite{Clark2020}, the authors suggest that the the goal of AI systems dating back to the 1950s of soft theorem proving IN NATURAL LANGUAGE has been finally achieved and in a single forward pass (no intermediate steps, chain of thought, or scratchpad) of a Transformer encoder. Moreover, they claim to observe strong evidence of depth/recursive generalization with Transformers trained on rule/fact sets requiring depth-3 inferences to prove easily generalizing to 5 without practice. This seems like systematic, compositional reasoning with generalization to unseen depths may be solved, without intermediate steps and chain of thought?
%
%Ok, but in "ReCOGS: How Incidental Details of a Logical Form Overshadow an Evaluation of Semantic Interpretation" \cite{Wu2023}, the authors discuss the COGS benchmark which so far has stumped state of the art models with literally 0\% accuracy on structural generalization splits, including length generalizations which seem analogous to \cite{Clark2020}'s depth generalizations. There is hope in that they are able to get traction for the models in a modified form of the task they call ReCOGS by removing redundant symbols and using Semantic Exact Match instead of Exact Match to avoid forcing the model to predict arbitrary variable labeling in logical forms (the ReCOGS authors also concatenated examples to ensure that overall string lengths seen at test time for recursion depth examples were not out of distribution). So now we see gaps in performance and even where performance is strong that it can be fragile in somewhat unpredictable ways. It would seem we need a framework to at least be able to prove when we are operating in safe territory and know what to guard against.
%
%Pulling back to the highest theoretical level, "What Formal Languages Can Transformers Express? A Survey" \cite{Strobl2024}, is useful to add context from formal language theory, computational complexity, circuit complexity theory, and a lot of supporting experimental papers which give us robust lower and upper bounds on what Transformers can do. They suggest we need intermediate steps (chain of thought or scratchpad) to be able to solve the hardest problems and tell us without intermediate steps some things we should not expect to be able to reliably do (includes some simple problems, like Parity for example, which is like counting negative signs or a bunch of negations in language on an expression, is not expected to be learned by vanilla Transformer encoders; though with soft attention it is theoretically possible with low confidence outputs).\footnote{"Overcoming a Theoretical Limitation of Self-Attention" \cite{Chiang2022} also confirm experimentally they cannot learn PARITY with a Transformer but artifically construct a soft attention Transformer that can just barely solve it with confidence that is O(1/n) where n is the input length. This is perhaps suprising that Transformers struggle with Parity so much because feedforward neural networks in general learn this task easily per e.g. "Learning Internal Representations by Error Propagation" by Rumelhart et al 1986, it appears to be a defect of the self-attention operation. I wonder if the geometric attention in \cite{Csordas2022} could improve this situation possibly if number of Transformer layers/depth is also guaranteed to exceed the number of negations (if used in an encoder only configuration for a fair comparison; Transformers can definitely learn Parity in encoder-decoder configuration with intermediate steps).} The references they point to for additional information, like \cite{delétang2023neuralnetworkschomskyhierarchy}, go further and state a longer list, noting that learning to simply duplicate an input string in general is infeasible for a Transformer encoder to reliably perform without intermediate decoding steps, let alone other tasks requiring systematic processing like binary addition, binary multiplication.
%
%But \cite{Strobl2024} only give bounds in terms of toy tasks like Parity and Majority, and they don't have any specific recommendations on how to modify a problem to make it amenable to Transformers (sometimes trivial modifications can make a problem tractable as we saw with COGS to ReCOGS). \textbf{Ideally we would like a framework which can take our task at hand and analyze it to predict whether a Transformer in general will do well at it, with some course of action provided for how a difficult task may be modified to make it more amenable to a Transformer, with the framework also telling us how many attention heads and layers will be needed for tasks which are learnable by Transformers.}
%
%Reviewing "Thinking Like Transformers" \cite{Weiss2021} at this point improves situation by introducing a programming language, Restricted Access Sequence Processing (RASP), where any program written in it can be "compiled" to concrete Transformer weights, allowing proof by construction and understanding of how various steps in a program correspond to operations by Transformers. They also predict how many Transformer layers and attention heads are required on a per problem basis.
%
%"What Algorithms can Transformers Learn? A Study in Length Generalization" \cite{Zhou2024} then apply \cite{Weiss2021}'s RASP language to return to the inconsistent findings regarding generalization and use RASP to predict exactly which cases of generalization come easily to Transformers and which not. \textbf{\cite{Zhou2024} turn \cite{Weiss2021} into the framework we seek by demonstrating how to apply RASP to Transformer decoders with intermediate steps, and showing how to modify tasks like Parity and long addition in seemingly incidental ways based on RASP analysis to make them readily learnable by Transformers in a compositional, length generalizing way!} Specifically they add index hints to the addition task based on a RASP-grounded analysis of what is preventing the Transformer from learning it. They also observe that if multi-digit carries are not part of the training set one can still get generalization by making the carry causal for the causal autoregressive Transformer decoder mode by reversing the digits (least significant digit first), and prove this resolves the issue.
%

\section{Data}

COGS \cite{KimLinzen2020} and ReCOGS \cite{Wu2023} datasets were used as provided by the repository associated with \cite{Wu2023}\footnote{\begin{footnotesize}
https://github.com/frankaging/ReCOGS
\end{footnotesize}},
with special attention on the structural generalization splits (especially prepositional phrase Object-to-Subject modification).

The grammar and vocabulary description for COGS/ReCOGS English input sentences provided in the utilities associated with the IBM CPG project \cite{klinger2024compositionalprogramgenerationfewshot}\footnote{\begin{footnotesize}https://github.com/IBM/cpg/blob/
c3626b4e03bfc681be2c2a5b23da0b48abe6f570
/src/model/cogs\_data.py\#L523
\end{footnotesize}}
were used in designing our RASP solution and analyzing the ways in which this task could be learned (we did not actually use their grammar though, and our RASP solution is flat and non-hierarchical unlike their description of the COGS probabilistic context-free grammar which is hierarchical and recursive). See Figure \ref{data-and-task-figure}.
\section{Model}
\label{model}

We used the RASP interpreter of \cite{Weiss2021} to run our program. For RASP model design and details see Appendix \ref{rasp-word-level-model-design}.

We use word-level tokens for all results in this paper,\footnote{
\begin{footnotesize}
We believe any solution at the word-level can be converted to a character-level token solution and that is not the focus of our investigation here (see Appendix \ref{rasp_character_level_model_notes} for proof of concept details on a character level solution not used here).
\end{footnotesize}
}
with an "embedding" layer that tags with possible part of speech\footnote{\begin{footnotesize}Note we follow the \cite{klinger2024compositionalprogramgenerationfewshot} description of COGS and include in our RASP vocabulary (part-of-speech or verb-type embedding/mapping) all words occurring anywhere in the upstream (Re)COGS "train.tsv" (including "exposure" rows, though would not change results qualitatively to omit the very few words only occurring in exposure examples). We also include two words in our vocab/embedding as common nouns accidentally left out of train.tsv vocabulary by the COGS author: "monastery" and "gardner" (only included in their train\_100.tsv and dev.tsv not also in train.tsv, but present in test/gen), a decision affecting just 22 or 0.1\% of generalization examples so would not affect any conclusions qualitatively. See also the discussion on COGS Github with the COGS author at https://github.com/najoungkim/COGS/issues/2\#issuecomment-976216841 .\end{footnotesize}}, and apply just once per encoder pass 19 attention-head compatible flat pattern-matching rules (Figures \ref{rasp-model-flat-pattern-match-example}, \ref{rasp-model-flat-pattern-match-example-v_dat_p2_match_with_middle_pp}, \ref{rasp-model-flat-pattern-match-example-v_dat_p2_non_matching}), shown using grammar coverage \cite{fuzzingbook2023:GrammarCoverageFuzzer} to be learnable from the training data\footnote{\begin{footnotesize}Specific training examples for each rule are in Table \ref{specific-grammar-pattern-examples-mapped-to-part-of-speech-and-cogs-in-distribution-training-examples} the end of "Appendix: Grammar Coverage analysis to develop and justify Restricted Access Sequence Processing model design" (\ref{grammar_coverage_analysis_for_model_design})\end{footnotesize}}, plus general pp/cp handling logic.
% , and outputs the next LF token (repeating until the LF is complete). 

Consistent with \cite{Zhou2024} we use \cite{Weiss2021}'s RASP originally used for modeling Transformer encoders to model an encoder-decoder in a causal way by feeding the autoregressive output back into the program.\footnote{\begin{footnotesize}We only have aggregations with non-causal masks when that aggregation (or without loss of generality just before the aggregation product is used to avoid multiplying everywhere) is masked by an input mask restricting it to the sequence corresponding to the input.\end{footnotesize}}

For training Transformers from scratch with randomly initialized weights, we use scripts derived from those provided by \cite{Wu2023}\footnote{
\begin{footnotesize}https://github.com/frankaging/ReCOGS/blob/
1b6eca8ff4dca5fd2fb284a7d470998af5083beb/run\_cogs.py

and

https://github.com/frankaging/ReCOGS/blob/
1b6eca8ff4dca5fd2fb284a7d470998af5083beb
/model/encoder\_decoder\_hf.py 
\end{footnotesize}
}. See "Appendix: Model Detail" (\ref{model_detail}).
\section{Methods}
We use the RASP \cite{Weiss2021} interpreter\footnote{\begin{footnotesize}provided at https://github.com/tech-srl/RASP/
\end{footnotesize}
} to evaluate our RASP programs\footnote{\begin{footnotesize}https://github.com/willy-b/learning-rasp/blob/2be461ba83b7393fda4dc5a96ae600ac98478771
/word-level-pos-tokens-recogs-style-decoder-loop.rasp with a demo at https://github.com/willy-b/learning-rasp/blob/
2be461ba83b7393fda4dc5a96ae600ac98478771
/recogs\_examples\_in\_rasp.py  
\end{footnotesize}
}. Logical forms (LFs) generated by the models were scored by Semantic Exact Match\footnote{\begin{footnotesize}Using the official scripts at https://github.com/frankaging/ReCOGS/blob/
1b6eca8ff4dca5fd2fb284a7d470998af5083beb/utils/train\_utils.py

and

https://github.com/frankaging/ReCOGS/blob/
1b6eca8ff4dca5fd2fb284a7d470998af5083beb/utils/compgen.py
\end{footnotesize}
} against ground truth.

We also measure grammar coverage \cite{fuzzingbook2023:GrammarCoverageFuzzer} (more detail in Appendix \ref{computing_grammar_coverage}) by input examples supported by our RASP model against the full grammar of COGS/ReCOGS input sentences provided in the utilities of the IBM CPG project \cite{klinger2024compositionalprogramgenerationfewshot}\footnote{\begin{footnotesize}https://github.com/IBM/cpg/blob/
c3626b4e03bfc681be2c2a5b23da0b48abe6f570
/src/model/cogs\_data.py\#L523 
\end{footnotesize}
}. See "Appendix: Methods Detail" (\ref{methods_detail}). See also "Appendix: Results Notebook links by section" (\ref{results_notebook_links_by_section}) for notebooks documenting results and giving steps to reproduce.
\section{Results}
\textbf{Restricted Access Sequence Processing - grammar coverage using a flat pattern matching approach (not tree-based and not recursive) and autoregressive decoder loop}

See "Appendix: Grammar Coverage Analysis for Design of Restricted Access Sequence Processing Model" (\ref{grammar_coverage_analysis_for_model_design}) for more details.

We generated 21 sentences based on rules present in the training examples\footnote{\begin{footnotesize}
See the equivalent official ReCOGS examples for each sentence above at Table \ref{specific-grammar-pattern-examples-mapped-to-part-of-speech-and-cogs-in-distribution-training-examples} the end of "Appendix: Grammar Coverage analysis to develop and justify Restricted Access Sequence Processing model design" (\ref{grammar_coverage_analysis_for_model_design}).
\end{footnotesize}} which cover 100\% of the COGS input grammar (lexical differences ignored, under the context-free grammar, tree-based assumption which is violated for our non-tree non-recursive model for prepositional phrases, requiring an additional rule to avoid "attraction errors", Figure \ref{attraction_errors_figure}) per \cite{fuzzingbook2023:GrammarCoverageFuzzer}:

\begin{tiny}
\begin{verbatim}
# 19 examples for non-recursive grammar rules
"the girl was painted", 
"a boy painted",
"a boy painted the girl",
"the girl was painted by a boy",
"a boy respected the girl", 
"the girl was respected",
"the girl was respected by a boy",
"the boy grew the flower",
"the flower was grown",
"the flower was grown by a boy",
"the scientist wanted to read",
"the guest smiled",
"the flower grew",
"ella sold a car to the customer",
"ella sold a customer a car",
"the customer was sold a car",
"the customer was sold a car by ella",
"the car was sold to the customer by ella",
"the car was sold to the customer",

# 2 examples for recursive grammar rules
# (1 prepositional phrase example)
"a boy painted the girl in a house",
# (1 complement phrase example)
"the girl noticed that a boy painted the girl"
\end{verbatim}
\end{tiny}

The first 19 of those sentences are present in our RASP program code\footnote{\begin{footnotesize}https://github.com/willy-b/learning-rasp/blob/2be461ba83b7393fda4dc5a96ae600ac98478771/word-level-pos-tokens-recogs-style-decoder-loop.rasp\#L574\end{footnotesize}}
and each correspond to a set of RASP operations corresponding to attention operations in a Transformer to match a template corresponding to that sentence type\footnote{\begin{footnotesize}
see also "Appendix: Restricted Access Sequence Processing word-level token program/model design" (\ref{rasp-word-level-model-design})

\end{footnotesize}
}. Those 19 examples reflect the only rules for handling non-prepositional/non-complement phrase grammar rules
%\footnote{\begin{footnotesize}
%\end{footnotesize}
%} 
present in our RASP solution (which gets 100\% semantic exact match and string exact match on the \cite{Wu2023} ReCOGS\_pos test set, see next results section). See Figure \ref{rasp-model-flat-pattern-match-example} for example grammar pattern matching with mismatched sentences and pp modification handling shown in additional figures in Appendix \ref{rasp-word-level-model-design}. To handle prepositional phrases in a flat solution, we find it necessary on the training data to add a rule that ignores noun phrases preceded by a prepositional phrase when searching for noun indexes to report in relationships (agent, theme, recipient, etc), and to loosen verb type templates to allow a gap for any prepositional phrase to be inserted. Considering how a model without this rule would behave led us to be able to predict 96\% of a certain category of errors a baseline Encoder-Decoder Transformer makes (see baseline attraction error results page \ref{error_analysis_for_baseline_transformer_predict_and_confirm_attraction_errors})).
%(740 out of 767; n=10 trained-from-scratch Transformers, most runs 100\% of that category for the Transformer) 

\begin{figure}
\includegraphics[scale=0.60]{./image_pdfs/example_rasp_for_recogs_flat_pattern_match.pdf}
\caption{\begin{small}Example RASP model flat grammar pattern matching, for np v\_dat\_p2 np np, for a matching sentence. See Figure \ref{rasp-model-flat-pattern-match-example-v_dat_p2_match_with_middle_pp} and \ref{rasp-model-flat-pattern-match-example-v_dat_p2_non_matching} in the Appendix for matching a sentence with middle-noun pp modification and non-matching cases.\end{small}}
\label{rasp-model-flat-pattern-match-example}
\end{figure}

\begin{table}
\centering
\begin{tabular}{p{0.6\linewidth} p{0.3\linewidth}}
\hline
\begin{tiny}\textbf{ReCOGS\_pos Split}\end{tiny} & \begin{tiny}\textbf{Semantic Exact Match \% (95\% CI)}\end{tiny} \\
\hline
\begin{tiny}ReCOGS\_pos test set (held out, in-distribution)\end{tiny} & \begin{tiny} 100.00\% (99.88-100.00\%)\end{tiny} \\
\hline
\begin{tiny}Generalization splits (held out, out-of-distribution) (below)\end{tiny} & \\
\hline
\begin{tiny}active\_to\_passive\end{tiny} & \begin{tiny}100.00\% (99.63-100.00\%)\end{tiny} \\
\begin{tiny}do\_dative\_to\_pp\_dative\end{tiny} & \begin{tiny} 100.00\% (99.63-100.00\%)\end{tiny} \\
\begin{tiny}obj\_omitted\_transitive\_to\_transitive\end{tiny} & \begin{tiny} 100.00\% (99.63-100.00\%)\end{tiny} \\
\begin{tiny}obj\_pp\_to\_subj\_pp\end{tiny} & \begin{tiny} 92.20\% (90.36-93.79\%)\end{tiny} \\
\begin{tiny}obj\_to\_subj\_common\end{tiny} & \begin{tiny} 100.00\% (99.63-100.00\%)\end{tiny} \\
\begin{tiny}obj\_to\_subj\_proper\end{tiny} & \begin{tiny} 100.00\% (99.63-100.00\%)\end{tiny} \\
\begin{tiny}only\_seen\_as\_transitive\_subj\_as\_unacc\_subj\end{tiny} & \begin{tiny} 100.00\% (99.63-100.00\%)\end{tiny} \\
\begin{tiny}only\_seen\_as\_unacc\_subj\_as\_obj\_omitted\_transitive\_subj\end{tiny} & \begin{tiny} 100.00\% (99.63-100.00\%)\end{tiny} \\
\begin{tiny}only\_seen\_as\_unacc\_subj\_as\_unerg\_subj\end{tiny} & \begin{tiny} 100.00\% (99.63-100.00\%)\end{tiny} \\
\begin{tiny}passive\_to\_active\end{tiny} & \begin{tiny} 100.00\% (99.63-100.00\%)\end{tiny} \\
\begin{tiny}pp\_dative\_to\_do\_dative\end{tiny} & \begin{tiny} 100.00\% (99.63-100.00\%)\end{tiny} \\
\begin{tiny}prim\_to\_inf\_arg\end{tiny} & \begin{tiny} 100.00\% (99.63-100.00\%)\end{tiny} \\
\begin{tiny}prim\_to\_obj\_common\end{tiny} & \begin{tiny} 100.00\% (99.63-100.00\%)\end{tiny} \\
\begin{tiny}prim\_to\_obj\_proper\end{tiny} & \begin{tiny} 100.00\% (99.63-100.00\%)\end{tiny} \\
\begin{tiny}prim\_to\_subj\_common\end{tiny} & \begin{tiny} 100.00\% (99.63-100.00\%)\end{tiny} \\
\begin{tiny}prim\_to\_subj\_proper\end{tiny} & \begin{tiny} 100.00\% (99.63-100.00\%)\end{tiny} \\
\begin{tiny}subj\_to\_obj\_common\end{tiny} & \begin{tiny} 100.00\% (99.63-100.00\%)\end{tiny} \\
\begin{tiny}subj\_to\_obj\_proper\end{tiny} & \begin{tiny} 100.00\% (99.63-100.00\%)\end{tiny} \\
\begin{tiny}unacc\_to\_transitive\end{tiny} & \begin{tiny} 100.00\% (99.63-100.00\%)\end{tiny} \\
\hline
\begin{tiny}all gen splits (19K examples, aggregate)\end{tiny} & \begin{tiny} 99.59\% (99.49-99.68\%)\end{tiny} \\
\hline
\end{tabular}
\caption{\begin{small}ReCOGS\_pos test set performance (n=3000) and non-recursive out-of-distribution generalization split performance for \textbf{Restricted Access Sequence Processing (RASP) Encoder-Decoder Transformer-compatible model} (n=1000 per gen split).\end{small}}%, Semantic Exact Match \%, with Beta/Clopper-Pearson confidence intervals. N=1000 examples for each generalization split. No examples excluded.\end{small}}
%This model gets 100\% Semantic Exact Match and String Exact Match on the in-distribution but unseen test set.
% Model uses a part-of-speech and verb-type map for word-level embedding and just 19 Transformer attention-head compatible flat grammar pattern recognizers based on rules visible in training data plus decoder-loop compatible prepositional phrase, complement phrase unrolling.\end{small}}
\label{results-table}
\end{table}

\textbf{ Restricted Access Sequence Processing - test set and generalization set performance}

The \textit{Restricted Access Sequence Processing program scored 100\% Semantic Exact Match and String Exact Match (no missed examples) (95\% confidence interval (Beta dist / Clopper-Pearson) of 99.88\% to 100\%, n=3000) on the ReCOGS\_pos test set}. The RASP model scored 99.59\% semantic exact match on all non-recursive out-of-distribution generalization splits (18922 out of 19000 (95\% confidence interval: 99.49\% to 99.68\%)). See Table \ref{results-table}.

Recursion splits are reported below.

\textbf{Restricted Access Sequence Processing - prepositional phrase and complement phrase recursion (tail recursive) with a non-tree, non-recursive approach using the decoder loop}\footnote{\begin{footnotesize}
The grammar includes two (tail) recursive aspects, prepositional phrase and complement phrase recursion.

The prepositional phrase recursion comes from the following COGS input grammar rules: `np -> np\_det | np\_prop | np\_pp` and `np\_pp -> np\_det pp np`.

Thus np can be expanded in an unbounded way as follows: `np -> (np\_det pp np) -> np\_det pp (np\_det pp np) -> np\_det pp np\_det pp (np\_det pp np)` and so on.

However, one sees this is tail recursion and can be handled by a loop that just appends `np\_det pp` until the final `np` is not `np\_pp`.

Complement phrase recursion 
%comes from the following COGS input grammar rules: 
arises from `np v\_cp\_taking that start`, 
%where the form supports being recursively expanded like 
which can recursively expand as `np v\_cp\_taking that start -> np v\_cp\_taking that (np v\_cp\_taking that start)`, and so on until the nonterminal start expands to some other non complement phrase related nonterminal.
\end{footnotesize}
}

\textit{Our RASP model's ReCOGS pp\_recursion AND cp\_recursion gen split scores were both 100\% semantic exact match AND string exact match (95\% confidence interval (Beta dist/Clopper-Pearson): 99.63\% to 100.0\%; n=1000 for each)}. See Table \ref{results-table}.

\textbf{\cite{Wu2023} Encoder-Decoder Transformer from scratch baselines (ReCOGS\_pos)}

\textit{\cite{Wu2023}'s baseline Encoder-Decoder Transformer on ReCOGS\_pos had an overall score of  88.55\% +/- 1.87\% Semantic Exact Match accuracy (sample +/- std, n=20) with a 95\% confidence interval for the sample mean when n=20 of 87.73\% to 89.37\%.}

\textit{\cite{Wu2023}'s baseline Encoder-Decoder Transformer's Semantic Exact Match score on the extremely difficult obj\_pp\_to\_subj\_pp split of ReCOGS\_pos was 19.7\% +/- 6.1\% Semantic Exact Match accuracy (sample +/- std, n=20) with 95\% confidence interval for the sample mean with n=20 of 17.0\% to 22.4\% .}

\textit{\cite{Wu2023}'s baseline Encoder-Decoder Transformer's Semantic Exact Match score on the pp\_recursion split of ReCOGS\_pos was 40.18\% +/- 2.07\% Semantic Exact Match accuracy (sample +/- std, n=20) with 95\% confidence interval for the sample mean with n=20 of 36.13 to 44.24\%.} \textit{\cite{Wu2023}'s baseline Encoder-Decoder Transformer's Semantic Exact Match score on the cp\_recursion split of ReCOGS\_pos was 52.40\% +/- 1.38\% Semantic Exact Match accuracy (sample +/- std, n=20) with 95\% confidence interval for the sample mean with n=20 of 51.80 to 53.01\%.}

\textbf{\cite{Wu2023} Encoder-Decoder baseline 2-layer Transformer does not improve on the obj\_pp\_to\_subj\_pp split when adding 1 or 2 additional layers}
%\footnote{\begin{footnotesize}
%This is consistent with the flat, non-tree structured hypothesis.
%we argue for in ths paper, that e.g. cannot learn to combine `np\_det pp np -> np\_pp -> np`, makes the predicted mistakes in the subj pp when the agent is left of the verb, and does poorly on our new v\_dat\_p2\_pp\_moved\_to\_recipient split  (see discussion).
%\end{footnotesize}
%}
\textbf{(even allowing parameter count to increase)}\footnote{\begin{footnotesize}
Since no improvement was observed, we did not run the costly experiments to increase the layers while controlling the parameter count (which would be a follow up to distinguish if the improvement was from the layer increase or the parameter increase).
\end{footnotesize}
}

3-layer \cite{Wu2023} Encoder-Decoder on ReCOGS\_pos obj\_pp\_to\_subj\_pp split: 16.2\% +/- 2.7\% Semantic Exact Match (sample mean +/- std, n=10) with 95\% confidence interval for sample mean (n=10) of 14.6\% to 17.9\% . 4-layer \cite{Wu2023} Encoder-Decoder on ReCOGS\_pos obj\_pp\_to\_subj\_pp split: 19.3\% +/- 4.1\% Semantic Exact Match (sample mean +/- std, n=10) with 95\% confidence interval for sample mean (n=10) of 16.8\% to 21.8\%.

\textbf{Attraction Error Analysis for \cite{Wu2023} baseline Encoder-Decoder Transformer on obj\_pp\_to\_subj\_pp split}

%To assess whether the mechanism of the errors in the obj\_pp\_to\_subj\_pp generalization split was what we expect from the Restricted Access Sequence Processing program, and were due to the Transformer naively matching the closest noun (now the prepositional phrase noun) instead of the original subject noun when the noun to the left of the verb was modified by a prepositional phrase, adding a prepositional phrase noun closer than the target noun, we analyzed the specific errors the \cite{Wu2023}'s Transformer made (see Appendix \ref{appendix_error_analysis_for_baseline_transformer_methods} for methods detail)

% TODO: move this block to methods and appendix on methods detail
%The errors from n=10 fresh training and evaluation runs of the baseline \cite{Wu2023} Encoder-Decoder Transformer on their ReCOGS\_pos train.tsv and tested on their unmodified gen.tsv were analyzed. All the input sentences and output logical forms as well as the ground truth logical forms were logged during the run. The input sentences were parsed by the Lark parser\footnote{https://github.com/lark-parser/lark} against the COGS input grammar which allowed categorizing each sentence by its verb type \footnote{Code to analyze the errors is at: 
%https://anonymous.4open.science/r/RASP-for-ReCOGS-16B9/supplemental_data/ReCOGS_Baseline_non_RASP_Transformer_ReCOGS_error_prediction_with_n=10_Transformers_trained_from_scratch_(predicting_the_details_of_error_in_logical_form_on_obj_pp_to_subj_pp_split).ipynb
%}. The author manually inspected each of verb type patterns and categorized them by the position of the agent and theme relative to the verb (see "Appendix: Error Analysis - parsing sentences with Lark and tagging sentences as agent left-of-verb or not" (\ref{appendix_error_analysis_for_baseline_transformer_methods})) as used below.

%For the error analysis of the difficult obj\_pp\_to\_subj\_pp split, considering only single verb cases\footnote{the 2 verb case of v\_inf and v\_inf\_taking are being analyzed and will be included} and ignoring sentences with complement phrases, of the sentences with the model generating an invalid logical form assessed by Semantic Exact Match, we focused on examples with a single error in one of the logical form parts (e.g. agent, theme, recipient, or nmod relationships).\footnote{Of the single relationship errors, we categorized them by a description of the position of both the agent and theme relative to the verb in that sentence (agent was considered to be either left OR "right or middle"; theme could be left, right, or middle) and what relationship had the error. Complement phrase examples were excluded to focus on predicting the form of the error on simpler examples.}

%Our hypothesis is in terms of nouns with a logical form relationship to a verb or other noun, where the relationship could be agent, theme, or recipient.
%Since the obj\_pp\_to\_subj\_pp split is in terms of subject vs object prepositional modification (instead of agent, recipient, or theme), we use the subset of sentences within this split where the agent is to the left of the verb and modified by a prepositional phrase as it corresponds to the subject in that case.
% end block to mostly move to methods

(For additional methods detail see Appendix (\ref{appendix_error_analysis_for_baseline_transformer_methods}).) Of the obj\_pp\_to\_subj\_pp split single part errors in single verb sentences made by the \cite{Wu2023} baseline Encoder-Decoder Transformer where the agent was to the left of the verb\footnote{\begin{footnotesize}
Our hypothesis is in terms of nouns with a logical form relationship to a verb or other noun, where the relationship could be agent, theme, or recipient.
Since the obj\_pp\_to\_subj\_pp split is in terms of subject vs object prepositional modification (instead of agent, recipient, or theme), we use the subset of sentences within this split where the agent is to the left of the verb and modified by a prepositional phrase as it corresponds to the subject in that case.
\end{footnotesize}
}, 
across n=10 models\footnote{\begin{footnotesize}
On a per model basis (n=10), the fraction of agent-left single point errors where it was the agent relationship in the logical form that was broken were: [0.985, 1.0, 1.0, 1.0, 1.0, 0.990, 1.0, 1.0, 1.0, 1.0].
\end{footnotesize}
}, 765 out of 767 (99.74\%; 95\% confidence interval 99.06 to 99.97\%) were in the agent part of the logical form (the predicted position for the error).

\textit{Critically across all n=10 \cite{Wu2023} models, for 96.73\% (740 out of the previously mentioned 765 above; 95\% confidence interval (Beta dist / Clopper-Pearson) 95.21 to 97.87\%) of the single point errors in logical forms for single verb sentences where the agent was on the left, modified by a prepositional phrase, and the error was in the agent part, the error in the logical form was that the agent index was accidentally assigned to the specific expected prepositional phrase noun (the one closest to the verb on the left side) instead of the original agent noun.} (Figure \ref{attraction_errors_figure})
This does not vary much from randomly initialized and trained model to model, with the model-level average at 97.07\% of such errors exactly as predicted (stderr=2.23\% (n=10)), with 7 of 10 models having 100\% of these errors exactly as predicted by our hypothesis\footnote{\begin{footnotesize}
Fraction for each model as predicted: [0.970, 0.761, 1.0, 
1.0, 1.0, 1.0, 1.0, 1.0, 0.976, 1.0]).
\end{footnotesize}
}
The attraction to the nearest noun hypothesis predicts that the offset in the agent index varies with prepositional phrase recursion depth (as at depth > 1, there are multiple attractor prepositional nouns to choose from).
\footnote{\begin{footnotesize}
See "Appendix: Attraction errors" (\ref{attraction_errors}) for examples of different pp recursion depths.
\end{footnotesize}
}

%Since the confidence that the attraction to the nearest holds in the uncommon pp depth greater than 1 case depends on the number of examples we were able to observe, 
We report that for all (n=22) single logical form part errors observed (from running n=10 separate Transformer models over the 1000 sentences in the split) where in the input the agent is left of the verb and has a depth=2 prepositional phrase modification in this split, in 100\% (95\% confidence interval (Beta dist / Clopper-Pearson) 84.6 to 100\%; n=22) of those sentences the error in the agent right-index matched our prediction.\label{error_analysis_for_baseline_transformer_predict_and_confirm_attraction_errors}

\textbf{\cite{Wu2023} Encoder-Decoder Transformer on new v\_dat\_p2 pp moved to recipient (from theme) split - as hard as hardest previous generalization split}

\textbf{See Figure} \ref{baseline_transformer_standard_training_cannot_do_v_dat_p2_generalization} . As the Restricted Access Sequence Processing program predicted the `np v\_dat\_p2 np pp np np`\footnote{\begin{footnotesize}
Strictly speaking we only do `np v\_dat\_p2 np\_det pp np np` as per the grammar `np\_prop` cannot precede a prepositional phrase
\end{footnotesize}
} prepositional phrase modification (which involves the recipient instead of the subject so is a distinct check of our hypothesis) would require learning to ignore the distractor "pp det common\_noun" and "pp proper\_noun" same as required for the obj\_pp\_to\_subj\_pp split, we predicted that a new split we introduce "v\_dat\_p2\_pp\_moved\_to\_recipient" would also be difficult for the Transformer. To test this, \cite{Wu2023}'s baseline Encoder-Decoder Transformer was trained with default data (ReCOGS\_pos train.tsv) and tested on modified v\_dat\_p2 pp training examples where only the word order was changed to move the prepositional phrase from the theme to the recipient (logical form properly updated see Appendix \ref{v_dat_p2_recipient_pp-modification_for_generalization_assessment_and_data_augmentation_attempt} for all examples). \textit{The baseline \cite{Wu2023} Encoder-Decoder Transformer was only able to achieve a Semantic Exact Match (sample mean +/- sample std) of 13\% +/- 15.6\% (n=10 Transformers trained from scratch with randomly initialized weights and data shuffling) with a 95\% confidence interval for the sample mean when n=10 of 4\% to 23\%}. Thus, this new split we introduce here as v\_dat\_p2\_pp\_moved\_to\_recipient is as difficult or perhaps more difficult than the previous reported "hardest split" obj\_pp\_to\_subj\_pp.

% Note, I want to explicitly add a test split for the RASP model evaluation to report this as well (not just the other pp generalization split noted to be difficult by \cite{Wu2023} which we show it handles, obj_pp_to_subj_pp), though the rules should cover this completely (how this was handled while writing the RASP model gave me the idea to test the Transformer trained from scratch in the first place, but reporting the performance on all of the same examples would be good for consistency)

\textbf{\cite{Wu2023} Encoder-Decoder Transformer trained with data augmented with v\_dat\_p2 pp moved to recipient (from theme) does NOT improve obj\_pp\_to\_subj\_pp performance}

\cite{Wu2023}'s baseline Encoder-Decoder Transformer was trained with default data (ReCOGS\_pos train.tsv) but with additionally the same modified v\_dat\_p2 pp training examples used for the "v\_dat\_p2\_pp\_moved\_to\_recipient" split (non-subject recipient modified with prepositional phrase, so nonoverlapping with subj\_pp, Figure \ref{baseline_transformer_standard_training_cannot_do_v_dat_p2_generalization} (b) and (d)) above on which it performed poorly, then tested on the standard prepositional modification generalization split "obj\_pp\_to\_subj\_pp", after which it achieved 22\% +/- 6.7\% Semantic Exact Match (sample mean +/- std, n=10) with 95\% confidence interval for sample mean n=10 of 17.9\% to 26.1\% (not significantly different than \cite{Wu2023}'s baseline by one-tailed Welch's unequal variances t-test). 

\section{Analysis} 
Our RASP model of a Transformer Encoder Decoder, without tree-based or recursive aspects scored 100\% in semantic exact match accuracy on the \cite{Wu2023} test set (n=3000), and on the generalization data scored 100\% in all but one category (see above) without explicit rules in the RASP program to handle them. This includes 100\% semantic exact match accuracy on the prepositional phrase recursion and complement phrase recursion generalization splits up to depth 12 (n=1000 examples each), without any hardcoded prepositional phrase or complement phrase expansion shortcuts added\footnote{\begin{footnotesize}
a single rule applies to all depths; the only limit on length generalization is the RASP interpreter and a simple to extend positional encoding which only handles sentences up to a limit
%due to a map that only covers numbers up to a limit but can be easily expanded by literally adding dummy entries like "'121':0,'122':0" and so on to a map in one place
\end{footnotesize}}. The RASP program only made a significant number of errors on obj\_pp\_to\_subj\_pp which scored only 92.20\% Semantic Exact Match (95\% confidence interval (Beta dist / Clopper-Pearson): 90.36\% to 93.79\%) Semantic Exact Match accuracy, much better than \cite{Wu2023} baseline Encoder-Decoder Transformers which only scored 19.7\% +/- 6.1\% Semantic Exact Match (sample mean +/- std) with 95\% confidence interval for the sample mean with n=20 of 17.0\% to 22.4\% (n=20 separately trained models with different random seeds for weight initialization and training data ordering; n=1000 examples used to test each of the n=20 models).

\textbf{Thus, we demonstrated by construction using the Restricted Access Sequence Processing language which can be compiled to concrete Transformer weights that theoretically a Transformer Encoder-Decoder can solve the COGS input to ReCOGS\_pos logical form translation in a systematic, compositional, and length generalizing way.}

Recall we found a single flat pattern matching rule we originally added to fit training examples, to ignore "pp det common\_noun" and "pp proper\_noun" when matching nouns for the agent, theme, recipient right indices, was sufficient to avoid structural generalization errors due to pp modification in novel positions. 

Interestingly, we imagined ablating that single rule and hypothesized attraction to the nearest noun (Figure \ref{attraction_errors_figure} and "Appendix: Attraction Errors" (\ref{attraction_errors}).)\footnote{\begin{footnotesize}
Specifically, a non-agent noun which was part of the actual attention-head compatible verb-centered pattern match (closer to the verb than the actual agent) in our RASP model getting labeled the agent. \textbf{See "Appendix: Attraction errors"} (\ref{attraction_errors}) \textbf{for a detailed figure and connection to prior NLP and psycholinguistics literature on similar errors in the context of hierarchical vs linear processing by language models and humans.}
\end{footnotesize}
} in its absence and found this predicted the exact error 
(the nearest noun to the verb is mistaken for the agent of the verb) 
in 96\% of the single relationship errors the \cite{Wu2023} baseline Transformers make on the obj-pp-to-subj-pp split\footnote{overall, their semantic exact match on the split is measured by us at 19.7\%, consistent with their Figure 5} when the agent is left of the verb in single verb sentences
%\footnote{\begin{footnotesize}
%%we also predicted a new split "v\_dat\_p2\_pp\_moved\_to\_recipient" based on "attraction" errors expected of the flat non-hierarchical, non-tree model and experimentally confirm it is just as difficult for the baseline Transformer.
%\end{footnotesize}} 
(suggesting perhaps that the baseline \cite{Wu2023} Transformer trained from scratch is also not learning a hierarchical, tree-structured representation.)\footnote{\begin{footnotesize}
We found \cite{li2023slogstructuralgeneralizationbenchmark} also observe 
%in their new benchmark SLOG, the error we predicted and confirmed here on ReCOGS ("attraction" error of closest noun (now prepositional phrase noun) being mistaken for subject/agent of verb when agent is left of verb and modified by pp), 
this stating "For instance, in sentences like 'A cat on the mat froze', models often misinterpret the closer NP the mat as the subject."
\end{footnotesize}
}

Our explanation for the \cite{Wu2023} baseline Encoder-Decoder Transformer errors could have been refuted by other single relationship errors occurring as frequently as the agent, indicating general model confusion (independently getting incorrect agent and theme, not just agent relationships) and/or when making an agent error, the model could have simply put nonsense indices or referred to any other position other than the closest pp noun position to the verb (which does vary and depends on pp depth) to refute our hypothesis.

The attraction to a nearer noun mechanism can also be checked by making a prediction on a completely different syntax affected by the same issue: the `np v\_dat\_p2 np pp np np` prepositional phrase modification (which involves the recipient relationship being modified instead of the subject and/or agent so is a distinct check of our hypothesis)\footnote{\begin{footnotesize}See Appendix \ref{v_dat_p2_recipient_pp-modification_for_generalization_assessment_and_data_augmentation_attempt}\end{footnotesize}} and we indeed found that this was as hard or harder than the previous most difficult split analyzed above, the `obj\_pp\_to\_subj\_pp` split.


Maybe \cite{Wu2023} baseline Encoder-Decoder is depth-constrained to learn a non-hierarchical, flat, non-tree model with these characteristic errors and with more layers it would learn to recursively combine `np\_det pp np -> np\_pp -> np` (to some fixed depth at least, probably limited by the number of Transformer blocks) and perform better on prepositional phrase related splits\footnote{\begin{footnotesize}
this is not a very scalable approach as we must make the network deeper to handle longer prepositional chains instead of just looping
\end{footnotesize}
}\footnote{\begin{footnotesize}
%We know that Transformers only move information between positions in a sequence at each layer boundary via cross/self-attention. 
\cite{Csordas2022}:
%argues the Transformer layers should be as deep as the deepest data dependency in the computational graph of the problem, in our case the parse tree, stating: 
"the network should be sufficiently deep, at least as deep as the deepest data dependency in the computational graph built from elementary operations (e.g., in the case of a parse tree, this is the depth of the tree)".
\end{footnotesize}
}. 

However, training a \cite{Wu2023} baseline Encoder-Decoder Transformer from scratch we found no benefit to 3 or 4 layers instead of 2 on the ReCOGS obj\_pp\_to\_subj\_pp split, consistent with \cite{petty2024impactdepthcompositionalgeneralization}'s finding on COGS.

Taken together, these results and the grammar coverage analysis suggest we may interpret the poor performance on generalizing on unseen prepositional phrase related modification related splits as arising from the baseline 2 to 4 layer Encoder-Decoder Transformers learning a flat representation (non-tree, non-recursive) that cannot leverage the grammar rule `np\_det pp np -> np\_pp -> np` during learning and which requires them to instead actually observe more of the various prepositional phrase substitutions to learn them.\footnote{\begin{footnotesize}See "Appendix: Composition and Learning" (\ref{composition_and_learning})\end{footnotesize}}.

\begin{figure}
\includegraphics[scale=0.30]{./image_pdfs/new_difficult_generalization_v_dat_p2_recipient_pp_modification_predicted_and_confirmed_for_transformers_trained_from_scratch_portrait.pdf}
\caption[\cite{Wu2023} baseline Encoder-Decoder Transformer trained from scratch generalizing to new v\_dat\_p2 pp moved to recipient (from theme) split is as hard as the previously reported hardest generalization split as predicted by the flat/non-recursive/non-tree hypothesis by RASP modeling]{\begin{small}\cite{Wu2023} Encoder-Decoder Transformer trained from scratch generalizing to new v\_dat\_p2 pp moved to recipient (from theme) split is as hard as the previously reported hardest generalization split as predicted by the flat/non-recursive/non-tree hypothesis by RASP modeling. See Figure \ref{rasp-model-flat-pattern-match-example-v_dat_p2_match_with_middle_pp} for how the flat RASP model would recognize example (b).\end{small} 
\label{baseline_transformer_standard_training_cannot_do_v_dat_p2_generalization}
}
%\textit{Actual COGS training example and modified version shown, which have different meanings but in a compositional solution are identical at a medium level of abstraction, as both are `np v\_dat\_p2 np np`\protect\footnotemark, so model should be able to translate both to their appropriate logical forms.}
\end{figure}
%\footnotetext{We want a model that generalizes from seeing a noun modified with a preposition in one position to others (e.g. by understanding the expansion `np -> np\_pp -> np\_det pp np` but the RASP program shows such a tree-based representation is not necessary)}
\section{Conclusion} 
Implementing our task in Restricted Access Sequence Processing immediately helped us discover additional related failure modes (e.g. new "v\_dat\_p2\_pp\_moved\_to\_recipient" split\footnote{\begin{footnotesize}
After this paper was written we found our predicted split "v\_dat\_p2\_pp\_moved\_to\_recipient" has also been added to an extended separate (SLOG) version of COGS (upstream of ReCOGS) recently in \cite{li2023slogstructuralgeneralizationbenchmark} see their section 2.2.1 indirect object modification (4) and confirmed by them as difficult as well
\end{footnotesize}
}) of \cite{Wu2023}'s baseline Encoder-Decoder Transformer, predict errors in detail made in the logical forms and may help us reason about why 2 layers is sufficient for the ReCOGS task\footnote{\begin{footnotesize}not surprising a shallow model can handle it as we have proved a flat, non-hierarchical approach is sufficient\end{footnotesize}}, and recommend others to consider to use RASP to understand Transformer behavior even for more complicated tasks like ReCOGS. We also predict that Transformers should be able to perform the ReCOGS task (even the structural generalization splits) with high accuracy and that the problem is just of getting the Transformer to learn the appropriate rules\footnote{\begin{footnotesize}e.g. to ignore "pp det common\_noun" and "pp proper\_noun" when finding nouns in relationships with verbs, which allows the RASP model to get 100\% on the ReCOGS test set, and 100\% two of the three structural generalizations, and 92\% on the obj-pp-to-subj-pp split\end{footnotesize}}, turning attention to data augmentation\footnote{\begin{footnotesize}
We tried one augmentation, see Results and Appendix \ref{v_dat_p2_recipient_pp-modification_for_generalization_assessment_and_data_augmentation_attempt}, but there are many other possibilities.
%We note we attempted in the course of this work one simple data augmentation (see Methods) which did not improve performance on the obj-pp-to-subj-pp split which was to attempt to teach the network to ignore the distracting "pp det common\_noun" and "pp proper\_noun" (a rule we had to add to the RASP, see Appendix \ref{rasp-word-level-model-design} and Model sections) which along with templates with dynamic size gaps (match any occurrence of something in the future rather than a fixed length away) allows our RASP model to achieve high obj-pp-to-subj-pp scores despite being a non-tree solution. The author will continue to attempt to augment the data to get the model to learn to ignore "pp det common\_noun" and "pp proper\_noun" when looking for nouns to assign to right indices of relationships, as the initial attempt did not appear to get the trained-from-scratch Transformer to learn this rule, which means we cannot conclude anything about whether that rule is sufficient in the trained as it is in the RASP model, and is also not sufficient to say that the Transformer won't be able to learn the rule with a simple data augmentation (we only tried one).
\end{footnotesize}
}, curriculum learning \cite{10.1145/1553374.1553380}, reinforcement learning \cite{Ranzato2015}, or other approaches.

\section*{Known Project Limitations}

The Restricted Access Sequence Processing code is not optimized. Cannot yet predict attention heads and layers required from the select and aggregate operations performed like the RASP authors \cite{Weiss2021} were able to do with their problems.

Grammar coverage \cite{fuzzingbook2023:GrammarCoverageFuzzer} is only valid when the expansions are rules your model can learn.\footnote{\begin{footnotesize}
If for example, as with our flat RASP model by design or as we hypothesize for \cite{Wu2023}'s baseline Encoder-Decoder Transformer, the model cannot or will not learn the rule `np\_det pp np -> np\_pp -> np` which recursively replaces noun phrases modified by prepositional phrases with a noun phrase, then grammar coverage will assume any prepositional phrase exposure is sufficient, which is evidently not true given the errors on prepositional phrase modification generalization splits reported here and by \cite{Wu2023}, \cite{KimLinzen2020}.
\end{footnotesize}
} We specifically made use of this limitation in this paper but still caution anyone about it who might just take the grammar coverage metric away by itself.

The attraction error analysis of the \cite{Wu2023} baseline Encoder-Decoder Transformer on the obj\_pp\_to\_subj\_pp split does not yet attempt to explain the common case of multiple errors in the logical form.\footnote{\begin{footnotesize}e.g. agent index may be replaced by prepositional phrase noun but also a spurious theme relationship is added or the theme index is also corrupted\end{footnotesize}}

Today, we only provide a RASP solution for ReCOGS \cite{Wu2023} here, not the semantically equivalent COGS\footnote{\begin{footnotesize}nor the recently introduced extended version SLOG \cite{li2023slogstructuralgeneralizationbenchmark}\end{footnotesize}}.

Much deeper Transformer networks may be learning a tree-based grammar representation and not suffer from the predicted generalization issues observed in \cite{Wu2023}'s baseline 2-layer Transformer and our intentionally non-tree RASP model.\footnote{\begin{footnotesize}
Nothing explored here rules that out and there is plenty of evidence outside the COGS task-related literature suggesting this will be the case: \cite{tenney2019bertrediscoversclassicalnlp} show the 24-layer BERT model seems to handle "POS tagging, parsing, NER, semantic roles, then coreference"; \cite{hewitt-manning-2019-structural} "provid[e] evidence that entire syntax trees are embedded implicitly in deep models’ [including BERT's] vector geometry", and \cite{goldberg2019assessingbertssyntacticabilities} shows BERT excels at subject-verb agreement, "which [is] traditionally taken as evidence for
the existence [of] hierarchical structure" (though e.g. in this work we see that ignoring distractor nouns in long-term dependencies does not require hierarchy or a deep understanding of syntax but simple rules like ignore "pp det common\_noun" and "pp proper\_noun" for finding noun-verb relationships in the logical form can allow for handling of such long-range dependencies). On the other hand, \cite{petty2024impactdepthcompositionalgeneralization} argue specifically for the COGS benchmark (semantically equivalent to ReCOGS which is derived from it) that increasing depth does not allow their Transformers to make progress on the structural generalization splits, even at depths up to 32 layers.
\end{footnotesize}
}

\bibliography{bibliography}

\section*{Notes}
No AI tools were used by the author in the preparation of this manuscript with the exception of anything used in the backend by Google Scholar searches for literature and citations and Google searches for related material. AI writing aids were not used.
\clearpage

\section{Appendix}

\subsection{Results Notebook links by section}
\label{results_notebook_links_by_section}

\textbf{ReCOGS RASP model on test set}
For steps to reproduce and results, see https://github.com/willy-b/RASP-for-ReCOGS/blob/
0d7cfb3b0721c17669193ac032a6c1a5b377d761
/supplemental\_data/RASP\_model\_for\_ReCOGS\_eval\_test\_set
\_(multiday\_run\_on\_dedicated\_VM)
\_(PR\_7\_contents\_on\_TEST\_set
\_incl\_complement\_phrase\_support)\_(public).ipynb .

\textbf{ReCOGS RASP model on generalization set (all splits)}
For steps to reproduce and results, see https://github.com/willy-b/RASP-for-ReCOGS/blob/
0d7cfb3b0721c17669193ac032a6c1a5b377d761
/supplemental\_data/
RASP\_model\_for\_ReCOGS\_eval\_on\_gen\_set\_(multiday\_run\_on\_dedicated\_VM)
\_(PR\_7\_contents\_on\_GEN\_set\_incl\_complement\_phrase\_support)\_(public).ipynb .

\textbf{\cite{Wu2023} Encoder-Decoder Transformer from scratch baselines (ReCOGS\_pos)}

See https://github.com/willy-b/RASP-for-ReCOGS/blob/
1f0c74914b893f356e1268015d7c463dccf97454
/supplemental\_data/RASP\_model\_for\_ReCOGS\_eval\_on\_gen\_set
\_(multiday\_run\_on\_dedicated\_VM)
\_(PR\_7\_contents\_on\_GEN\_set\_incl\_complement\_phrase\_support)\_(public).ipynb

for \cite{Wu2023} script execution and analysis code.

\textbf{\cite{Wu2023} Encoder-Decoder baseline 2-layer Transformer does not improve on the obj\_pp\_to\_subj\_pp split when adding 1 or 2 additional layers}

3 and 4 layer results can be found in:

https://github.com/willy-b/RASP-for-ReCOGS/blob/
1f0c74914b893f356e1268015d7c463dccf97454
/supplemental\_data/
RASP\_for\_ReCOGS\_(no\_RASP\_in\_this\_file)
\_more\_Wu\_et\_al\_2023\_transformer\_baselines
\_to\_compare\_with\_Restricted\_Access
\_Sequence\_Processing\_(\_use\_fixed\_positional
\_indices)\_and\_or\_data\_augmentation.ipynb .

\textbf{Attraction Error Analysis for \cite{Wu2023} baseline Encoder-Decoder Transformer on obj\_pp\_to\_subj\_pp split}

See https://github.com/willy-b/RASP-for-ReCOGS/blob/
609f4fff9e5b8ff2354081db617ad576bc7fb840
/supplemental\_data/
ReCOGS\_Baseline\_non\_RASP\_Transformer
\_ReCOGS\_error\_prediction\_with
\_n%3D10\_Transformers
\_trained\_from\_scratch
\_(predicting\_the\_details\_of\_error
\_in\_logical\_form\_on\_obj\_pp\_to\_subj\_pp\_split).ipynb .

\clearpage

\subsection{Vocabulary and Grammar}
\label{vocabulary_and_grammar}

The description of the COGS \cite{KimLinzen2020} input vocab and grammar from the utilities of \cite{klinger2024compositionalprogramgenerationfewshot}'s CPG project\footnote{\begin{footnotesize}
https://github.com/IBM/cpg/blob/
c3626b4e03bfc681be2c2a5b23da0b48abe6f570
/src/model/cogs\_data.py\#L523 
\end{footnotesize}
} was used as a reference by this project \textbf{(we did not use the CPG code or anything from the CPG project other than the following Lark-compatible description of the COGS PCFG grammar)}.

Note this paper is focused on the ReCOGS task but the input sentences for ReCOGS are identical to the COGS input sentences (only the output format / logical form is changed).

The following COGS input context-free grammar description (based on training data, see \cite{klinger2024compositionalprogramgenerationfewshot}) was used for computing grammar coverage (\ref{grammar_coverage_analysis_for_model_design}) and for RASP model design (though \textbf{the RASP model does NOT use a context-free grammar, and just uses 19 flat pattern matching operations to cover this grammar}, see Model (\ref{model}) or for more detail Appendix \ref{rasp-word-level-model-design}):
\begin{tiny}
\begin{verbatim}
start: 
    s1 | s2 | s3 | s4 | vp_internal
s1: np vp_external
s2: np vp_passive
s3: np vp_passive_dat
s4: np vp_external4
vp_external: 
   v_unerg | v_trans_omissible_p1
   | vp_external1 | vp_external2 
   | vp_external3 | vp_external5 
   | vp_external6 | vp_external7
vp_external1: v_unacc_p1 np
vp_external2: 
   v_trans_omissible_p2 np
vp_external3: 
   v_trans_not_omissible np
vp_external4: 
   v_inf_taking to v_inf
vp_external5: 
   v_cp_taking that start
vp_external6: 
   v_dat_p1 np pp_iobj
vp_external7: 
   v_dat_p2 np np
vp_internal: np v_unacc_p2
vp_passive: vp_passive1 | vp_passive2 
          | vp_passive3 | vp_passive4 
          | vp_passive5 | vp_passive6 
          | vp_passive7 | vp_passive8
vp_passive1: 
    was v_trans_not_omissible_pp_p1
vp_passive2: 
    was v_trans_not_omissible_pp_p2 
          by np
vp_passive3: 
    was v_trans_omissible_pp_p1
vp_passive4: 
    was v_trans_omissible_pp_p2 by np
vp_passive5: 
    was v_unacc_pp_p1
vp_passive6: 
    was v_unacc_pp_p2 by np
vp_passive7: 
    was v_dat_pp_p1 pp_iobj
vp_passive8: 
    was v_dat_pp_p2 pp_iobj by np
vp_passive_dat: 
    vp_passive_dat1
              | vp_passive_dat2
vp_passive_dat1: 
    was v_dat_pp_p3 np
vp_passive_dat2: 
    was v_dat_pp_p4 np by np
np: 
    np_prop | np_det | np_pp
np_prop: proper_noun
np_det: det common_noun
np_pp: np_det pp np
pp_iobj: to np
det: "the" | "a"
pp: "on" | "in" | "beside"
was: "was"
by: "by"
to: "to"
that: "that"
common_noun: 
    "girl" | "boy" 
    | "cat" | "dog" | ...
proper_noun: 
    "emma" | "liam" 
    | "olivia" | "noah" 
    | ...
v_trans_omissible_p1: 
    "ate" | "painted" | "drew" 
    | "cleaned" | ...
v_trans_omissible_p2: 
    "ate" | "painted" | "drew" 
    | "cleaned"  | ...
v_trans_omissible_pp_p1: 
    "eaten" | "painted" | "drawn" 
    | "cleaned" | ...
v_trans_omissible_pp_p2: 
    "eaten" | "painted" | "drawn" 
    | "cleaned" | ...
v_trans_not_omissible: 
    "liked" | "helped" | "found" 
    | "loved" | ...
v_trans_not_omissible_pp_p1: 
    "liked" | "helped" | "found" 
    | "loved"  | ...
v_trans_not_omissible_pp_p2: 
    "liked" | "helped" | "found" 
    | "loved" | ...
v_cp_taking: 
    "liked" | "hoped" | "said" 
    | "noticed" | ...
v_inf_taking: 
    "wanted" | "preferred" | "needed" 
    | "intended" | ...
v_unacc_p1: 
    "rolled" | "froze" | "burned" 
    | "shortened" | ...
v_unacc_p2: 
    "rolled" | "froze" | "burned" 
    | "shortened" | ...
v_unacc_pp_p1: 
    "rolled" | "frozen" | "burned" 
    | "shortened" | ...
v_unacc_pp_p2: 
    "rolled" | "frozen" | "burned" 
    | "shortened" | ...
v_unerg: 
    "slept" | "smiled" | "laughed" 
    | "sneezed" | ...
v_inf: 
    "walk" | "run" | "sleep" 
    | "sneeze" | ...
v_dat_p1: 
    "gave" | "lended" | "sold" 
    | "offered" | ...
v_dat_p2: 
    "gave" | "lended" | "sold" 
    | "offered" | ...
v_dat_pp_p1: 
    "given" | "lended" | "sold" 
    | "offered" | ...
v_dat_pp_p2: 
    "given" | "lended" | "sold" 
    | "offered" | ...
v_dat_pp_p3: 
    "given" | "lended" | "sold" 
    | "offered" | ...
v_dat_pp_p4: 
    "given" | "lended" | "sold" 
    | "offered" | ...
\end{verbatim}
\end{tiny}

For the vocabulary we follow the \cite{klinger2024compositionalprogramgenerationfewshot} description of COGS (same input as ReCOGS) and include in our RASP vocabulary (part-of-speech or verb-type embedding/mapping) all words occurring anywhere in the upstream (Re)COGS "train.tsv" (including "exposure" rows, though would not change results qualitatively to omit the very few words only occurring in exposure examples). We also include two words in our vocab/embedding as common nouns accidentally left out of train.tsv vocabulary by the COGS author: "monastery" and "gardner" (only included in their train\_100.tsv and dev.tsv not also in train.tsv, but present in test/gen), a decision affecting just 22 or 0.1\% of generalization examples so would not affect any conclusions qualitatively. See also the discussion on COGS Github with the COGS author at https://github.com/najoungkim/COGS/issues/2\#issuecomment-976216841 .
\clearpage

\subsection{Restricted Access Sequence Processing word-level token program/model design}
\label{rasp-word-level-model-design}

You can run a demo and see the autoregressive output 

\begin{tiny}
\begin{verbatim}
git clone https://github.com/willy-b/learning-rasp.git
python recogs_examples_in_rasp.py 
\end{verbatim}
\end{tiny}

The script will show performance on \cite{Wu2023} ReCOGS\_pos data by default, run with "--use\_dev\_split", "--use\_gen\_split" , or "--use\_test\_split" to see it run on those and give a running score every 10 rows.

For ReCOGS, intending to perform well on Semantic Exact Match, we took a simple, flat, non-tree, non-recursive approach which was able to get 100\% semantic exact match (and string exact match) on the full test set, and 99.6\% semantic exact match on the out-of-distribution generalization set of the real ReCOGS dataset\footnote{\begin{footnotesize} word-level token Restricted Access Sequence Processing solution: https://github.com/willy-b/learning-rasp/blob/2be461ba83b7393fda4dc5a96ae600ac98478771/word-level-pos-tokens-recogs-style-decoder-loop.rasp \end{footnotesize}}.

We took the RASP native sequence tokens, and first did a Transformer learned-embedding compatible operation and created 1 part-of-speech and 4 extra verb-type sequences (because each word in the COGS vocabulary may actually serve multiple POS roles; up to four different verb types as in the case of "liked" 

which can serve as v\_trans\_not\_omissible, v\_trans\_not\_omissible\_pp\_p1, v\_trans\_not\_omissible\_pp\_p2, and v\_cp\_taking types). 

The five extra sequences serve to associate each word with one or more of the following part-of-speech/verb type roles:
\begin{tiny}
\begin{verbatim}
det: 1
pp: 2
was: 3
by: 4
to: 5
that: 6
common_noun: 7
proper_noun: 8
v_trans_omissible: 9
v_trans_omissible_pp: 10
v_trans_not_omissible: 11
v_trans_not_omissible_pp: 12
v_cp_taking: 13
v_inf_taking: 14
v_unacc: 15
v_unerg: 16
v_inf: 17
v_dat: 18
v_dat_pp: 19
v_unacc_pp: 20
\end{verbatim}
\end{tiny}

Each of the five sequences comes from a separate map. Since in RASP a map could only have a single value per key, and since individual words had up to four different verb roles (as in "liked" which had 4).

Upon these five parallel, aligned, sequences we used a series of attention head compatible operations to recognize multi-token patterns (see below) corresponding to grammatical forms (listed below). 

\begin{tiny}
\begin{verbatim}
np_det_mask = select(7, pos_tokens, ==) 
and select(pos_tokens, 1, ==) 
and select(indices+1, indices, ==);
np_prop_mask = select(8, pos_tokens, ==) and 
select(indices, indices, ==);
np_det_sequence = aggregate(np_det_mask, 1);
np_prop_sequence = aggregate(np_prop_mask, 1);
np_det_after = select(np_det_sequence, 1, ==) and 
select(indices+1, indices, ==);
np_prop_after = select(np_prop_sequence, 1, ==) and 
select(indices+1, indices, ==);
np_after_mask = np_det_after or np_prop_after;
np_after_sequence = aggregate(np_after_mask, 1);
np_after_mask = select(np_after_sequence, 1, ==) and 
select(indices,indices, ==);
# ...

# np v_unerg
# e.g. [1,7,16]
set example ["the", "guest", "smiled"]
v_unerg_mask = select(16, pos_tokens_vmap1, ==) and 
select(indices, indices, ==);
np_v_unerg = aggregate(np_after_mask and v_unerg_mask, 1);
\end{verbatim}
\end{tiny}

\begin{figure}
\includegraphics[scale=0.73]{./image_pdfs/example_rasp_for_recogs_flat_pattern_match.pdf}
\caption{\begin{small}Example RASP model flat grammar pattern matching, for np v\_dat\_p2 np np, for a matching sentence.\end{small} \\
}
\label{rasp-model-flat-pattern-match-example-v_dat_p2_match}
\end{figure}
\clearpage

\begin{figure}
\includegraphics[scale=0.75]{./image_pdfs/example_rasp_for_recogs_flat_pattern_match_with_pp_modification.pdf}
\caption{\begin{small}Example RASP model flat grammar pattern matching, for np v\_dat\_p2 np np, for a matching sentence, despite pp modification of middle recipient noun. This is in the encoder. See also Figure \ref{attraction_errors_figure} for how the RASP model avoids attraction errors in assigning agents, recipients, themes due to prepositional phrase modification in the decoder.\end{small} \\
}
\label{rasp-model-flat-pattern-match-example-v_dat_p2_match_with_middle_pp}
\end{figure}
\clearpage

\begin{figure}
\includegraphics[scale=0.75]{./image_pdfs/example_rasp_for_recogs_flat_pattern_no_match.pdf}
\caption{\begin{small}Example RASP model flat grammar pattern matching, for the pattern np v\_dat\_p2 np np, for a non-matching sentence.\end{small} \\
}
\label{rasp-model-flat-pattern-match-example-v_dat_p2_non_matching}
\end{figure}
\clearpage

\textbf{These patterns are not causal because their use/input/output is masked to the input section of the sequence, so would take part in the Encoder of the Encoder-Decoder only}(\textbf{all operations outside the input mask in the word-level token RASP solution used in this paper are directly or indirectly causally masked} and we built symbol by symbol in a causal autoregressive way). \textbf{We could have added an explicit causal mask to each operation but for efficiency and simplicity of the code omitted it when we are doing it implicitly by taking only the last sequence position (we also acausally aggregate so that all sequence positions have the same value as the last sequence position to make it easier to read the output -- RASP interpreter will just print it as one position if they are all equal and we only take one position).}

Also, the author thinks many of these RASP steps could be consolidated. The goal here was to first prove by construction that a non-recursive, flat RASP program could get approximately 100\% Semantic Exact Match on all the ReCOGS generalization splits (we only missed one split by a little due to two week time constraint, insufficient time to add all rules).

Introduction of variables at the beginning of the ReCOGS logical form (e.g. in the logical form for "a boy painted the girl", we have "boy ( 1 ) ; * girl ( 4 ) ; paint ( 2 ) AND agent ( 2 , 1 ) AND theme ( 2 , 4 )" , the variable introduction is "boy ( 1 ) ; * girl ( 4 ) ; paint ( 2 )" before the "AND"). We took a simple approach and sorted the input sequence with nouns before verbs and determiners, fillers last (with determiners and fillers not having any corresponding entry in the output sequence). We then count nouns and verbs in the input and count nouns and verbs in the output and determine if we have introduced all the nouns and verbs.

(The sections below must be updated for PR\#7 which adds complement phrase support and complicated the approach somewhat) 

Example counting how many nouns and verbs we have output (introduced as variables) so far (to determine what we need to output for next token):
\begin{tiny}
\begin{verbatim}
nv_in_output_sequence = 
OUTPUT_MASK*(indicator(pos_tokens == 7 or pos_tokens == 8) + 
indicator(pos_tokens_vmap1 == 9 or pos_tokens_vmap2 == 10 or 
pos_tokens_vmap1 == 11 or pos_tokens_vmap2 == 12 or pos_tokens_vmap3 == 13 or 
pos_tokens_vmap4 == 14 or pos_tokens_vmap1 == 15 or pos_tokens_vmap1 == 16 or 
pos_tokens_vmap1 == 17 or pos_tokens_vmap1 == 18 or pos_tokens_vmap2 == 19 or 
pos_tokens_vmap2 == 20 or pos_tokens_vmap1==21));
nv_in_output_count = selector_width(select(nv_in_output_sequence, 1, ==)); 
# causal operation as we use only last sequence position
\end{verbatim}
\end{tiny}

How variables are introduced with their index (omitted sorting of input and other operations that can be read in the code and are less important; anything acausal is restricted to input sequence section (Encoder)):
(only value at last sequence position is used; causal)
\begin{tiny}
\begin{verbatim}
# introducing variables
output = "";
# definite article word handling
before_target_word_index = 
 aggregate(select(indices, nv_in_output_count, ==), 
 input_indices_sorted)-1;
has_star = 
 aggregate(select(indices, before_target_word_index, ==), 
 tokens) == "the";
last_output_is_star = 
 aggregate(select(indices, length-1, ==), 
 tokens) == "*";

input_nv_sorted_by_type = 
 input_tokens_sorted_by_type * 
 (input_noun_mask_sorted + input_verb_mask_sorted);
target_word_token = 
 aggregate(select(indices, nv_in_output_count, ==), 
 normalize_nv(input_nv_sorted_by_type)) 
if (not has_star or last_output_is_star) else "*";
# subtract 1 when matching 
# for producing the index 
# because we just output the additional word by then
target_word_index = 
 aggregate(select(indices, nv_in_output_count-1, ==), 
 input_indices_sorted);

output = 
 target_word_token 
 if ((num_tokens_in_output_excluding_asterisks % 5) == 0) 
 else 
 output;
output = 
 "(" 
 if ((num_tokens_in_output_excluding_asterisks % 5) == 1)
 else output;
output = 
 target_word_index
 if ((num_tokens_in_output_excluding_asterisks % 5) == 2) 
 else output;
output = 
 ")" 
 if ((num_tokens_in_output_excluding_asterisks % 5) == 3) 
 else output;
# note that 
# when nv_in_output_count == nv_in_input_count, 
# we will add AND instead of ";"
output = 
 (
  ";" 
  if 
  (
   5 * nv_in_input_count - 1 > 
   num_tokens_in_output_excluding_asterisks
  ) 
  else "AND"
 ) 
if (num_tokens_in_output_excluding_asterisks % 5 == 4) 
else output;

# if we didn't have an input/output separator 
# that needs to be output
output = 
 "|" if num_pipes_in_output == 0 else output;
\end{verbatim}
\end{tiny}
Note that "normalize\_nv" is a lookup into a map that has no effect unless the word is a verb in which case it normalizes it to a standard suffix ("ate" to "eat" , "painted" to "paint", etc).
\clearpage
As you can see above, if we have not introduced all the variables, we determine our index into the sorted list of nouns and verbs (nouns before verbs), and using a MLP modeling modulus, compute index mod 5 and condition on that to output that noun/verb or parentheses or index as prediction for next token at last sequence position (all other sequence positions are ignored). Since we do ReCOGS\_pos (semantically identical to random indices but avoid requiring random numbers) the index we use is the index of the original noun or verb in the original sequence. If we are still introducing variables, that is the end and we have our prediction for the next token.

If we are done introducing variables at that point in the decoder loop, we move on, 
and attention head compatible operations recognize templates in the five parallel part-of-speech / verb-type per location sequences for "v\_trans\_omissible\_p1", "v\_trans\_omissible\_p2", "v\_trans\_omissible\_pp\_p1", "v\_trans\_omissible\_pp\_p2", "v\_trans\_not\_omissible", "v\_trans\_not\_omissible\_pp\_p1", "v\_trans\_not\_omissible\_pp\_p2", "v\_cp\_taking", "v\_inf\_taking", "v\_unacc\_p1", "v\_unacc\_p2", "v\_unacc\_pp\_p1", "v\_unacc\_pp\_p2", "v\_unerg", "v\_dat\_p2", "v\_dat\_pp\_p1", "v\_dat\_pp\_p2",  "v\_dat\_pp\_p3",  "v\_dat\_pp\_p4".

Here are a couple of examples of patterns, to see how it looks if we support 1 verb pattern per input (no complement phrase recursion; which can be easily handled how we handle other things we loop over, looping over current phrase and masking and processing), which is sufficient to get approximately 100\% on all entries that do not use complement phrases (e.g. "so-and-so noticed that (full input here)"):
\begin{tiny}
\begin{verbatim}
# define the pattern
# ... \
# (just showing one example, 
#  np_prop_mask and 
#  np_before_mask 
#  are attention masks defined earlier)
# np v_dat_p2 np np
# e.g. [8,18,1,7,1,7]
set example ["ella","sold","a","customer","a","car"]
np_np_sequence = \
  aggregate((np_prop_mask and np_before_mask) or (np_det_left_mask and np_two_before_mask), 1);
# would not support prepositional phrase modification on middle NP
#np_np_before_mask = \
#  select(np_np_sequence, 1, ==) and select(indices-1, indices, ==);
np_np_any_before_mask = \
  select(np_np_sequence, 1, ==) and select(indices, indices, >); # acausal is ok \
# in INPUT sequence (encoder part, not decoder), \
# would mask further if we wanted to do multiple templates per input or \
# something outside the supported grammar (COGS without complement phrase \
# recursion is supported here)
np_np_any_before_sequence = \
  aggregate(np_np_any_before_mask, 1);
np_np_any_before_mask = \
  select(np_np_any_before_sequence, 1, ==) and select(indices, indices, ==);
np_v_dat_p_np_np = \
  aggregate(np_after_mask and v_dat_mask and np_before_mask and np_np_any_before_mask, 1);
# Example: np_v_dat_p_np_np(\
#  ['ella', 'sold', 'a', 'customer', 'a', 'car']) \
#  = [0, 1, 0, 0, 0, 0] (ints)
# Example: np_v_dat_p_np_np(\
#  [8, 18, 1, 7, 1, 7]) \
#  = [0, 1, 0, 0, 0, 0] (ints)

# ...

# check the pattern and set the template name
any_np_v_trans_omissible = \
  aggregate(select(np_v_trans_omissible, 1, ==), 1);
template_name = "v_trans_omissible_p1" 
if (any_np_v_trans_omissible == 1) else template_name;

# ...

any_v_dat_p2 = aggregate(select(np_v_dat_p_np_np, 1, ==), 1);
template_name = \
  "v_dat_p2" if (any_v_dat_p2 == 1) else template_name;

# ...

any_v_dat_pp_p4 = \
  aggregate(select(np_was_v_dat_pp_np_by_np, 1, ==), 1);
template_name = \
  "v_dat_pp_p4" if (any_v_dat_pp_p4 == 1) else template_name;

# must be checked after P4
any_v_dat_pp_p2 = \
  aggregate(select(np_was_v_dat_pp_to_np_by_np, 1, ==), 1);
template_name = \
  "v_dat_pp_p2" if (any_v_dat_pp_p2 == 1) else template_name;

# template name is used to lookup 
# the number of verb relationships to output
# and what they are for each index
# e.g. ["theme", "agent"] 
# vs. ["agent", "recipient", "theme"] etc
\end{verbatim}
\end{tiny}
\clearpage

The rest of this applies to just values used from the last sequence location (output is prediction for next symbol).

Based on the template recognized, we lookup the template size for number of relationships (theme, recipient, agent) for that verb type:
\begin{tiny}
\begin{verbatim}
def template_size(template_name) {
  template_sizes = {
 "": 0,
 "v_trans_omissible_p1": 1,
 "v_trans_omissible_p2": 2,
 "v_trans_omissible_pp_p1": 1,
 "v_trans_omissible_pp_p2": 2,
 "v_trans_not_omissible": 2,
 "v_trans_not_omissible_pp_p1": 1,
 "v_trans_not_omissible_pp_p2": 2,
 "v_cp_taking": 2,
 "v_inf_taking": 4,
 "v_unacc_p1": 2,
 "v_unacc_p2": 1,
 "v_unacc_pp_p1": 1,
 "v_unacc_pp_p2": 2,
 "v_unerg": 1,
# "v_inf": 1,
 "v_dat_p1": 3,
 "v_dat_p2": 3,
 "v_dat_pp_p1": 2,
 "v_dat_pp_p2": 3,
 "v_dat_pp_p3": 2,
 "v_dat_pp_p4": 3 
  };
  # v_inf_taking includes v_inf and an extra verb is why it is 4 instead of 2
  return template_sizes[template_name];
}
\end{verbatim}
\end{tiny}

Details are in the code, but we compute at the last sequence position (in parallel) the number of relationships output for the verb so far, and for the current relationship which token within that multi-token process (e.g. the word "agent" or the open parenthesis "(" or the left index, or the comma, or right index, close parenthesis ")", "AND", etc) we are on.

Like we computed at the last sequence position the number of nouns and verbs in the output once we are finished introducing nouns and verbs (this would be a little different with complement phrases (see repo for CP support)), we compute the number of agent,theme,recipient,xcomp,ccomp entries in the output:
\begin{tiny}
\begin{verbatim}
atrxc_in_output_sequence = OUTPUT_MASK*(indicator(tokens == "agent" 
or tokens == "theme" 
or tokens=="recipient" 
or tokens=="xcomp" or tokens =="ccomp"));
# agent_theme_recipient_xcomp_ccomp_output_count is the number of relationships we have output
agent_theme_recipient_xcomp_ccomp_output_count = 
selector_width(select(atrxc_in_output_sequence, 1, ==));
after_intro_idx = 
(nv_in_output_count - nv_in_input_count + \
agent_theme_recipient_xcomp_ccomp_output_count) \
if nv_in_output_count >= nv_in_input_count else 0;
after_intro_num_tokens_in_output_excluding_asterisks = 
num_tokens_in_output_excluding_asterisks - ((5 * nv_in_input_count));
\end{verbatim}
\end{tiny}
\clearpage
We use all those different values which are computed independently from the input sequence and so do not add depth/layer requirements as many of the ones which involve accessing the sequence can be done in parallel. 
We then use the verb-type and relationship index to the relationship into a map to get the current relationship to output (as some verb types output agent first, some output theme, etc):
\begin{tiny}
\begin{verbatim}
template_mapping1 = {
 "": "",
 "v_trans_omissible_p1": "agent",
 "v_trans_omissible_p2": "agent",
 "v_trans_omissible_pp_p1": "theme",
 "v_trans_omissible_pp_p2": "theme",
 "v_trans_not_omissible": "agent",
 "v_trans_not_omissible_pp_p1": "theme",
 "v_trans_not_omissible_pp_p2": "theme",
 "v_cp_taking": "agent",
 "v_inf_taking": "agent",
 "v_unacc_p1": "agent",
 "v_unacc_p2": "theme",
 "v_unacc_pp_p1": "theme",
 "v_unacc_pp_p2": "theme",
 "v_unerg": "agent",
 "v_inf": "agent",
 "v_dat_p1": "agent",
 "v_dat_p2": "agent",
 "v_dat_pp_p1": "theme",
 "v_dat_pp_p2": "theme",
 "v_dat_pp_p3": "recipient",
 "v_dat_pp_p4": "recipient"
};
\end{verbatim}
\end{tiny}

Outputting the verb relationships we must skip over any "pp np" as possible agents, themes, or recipients to avoid getting confused by noun phrases added by prepositional modification (believed by the author to be the cause of the issue with obj pp to subj pp generalization by \cite{Wu2023}'s Transformer).

\begin{tiny}
\begin{verbatim}
pp_sequence = indicator(pos_tokens == 2);
pp_one_after_mask = select(pp_sequence, 1, ==) and select(indices+1, indices, ==);
pp_one_after_sequence = aggregate(pp_one_after_mask, 1);
pp_one_after_mask = select(pp_one_after_sequence, 1, ==) and select(indices, indices, ==);

pp_two_after_mask = select(pp_sequence, 1, ==) and select(indices+2, indices, ==);
pp_two_after_sequence = aggregate(pp_two_after_mask, 1);
pp_two_after_mask = select(pp_two_after_sequence, 1, ==) and select(indices, indices, ==);

np_det_diag_mask = select(aggregate(np_det_mask, 1), 1, ==) and select(indices, indices, ==);
np_prop_diag_mask = select(aggregate(np_prop_mask, 1), 1, ==) and select(indices, indices, ==);

no_pp_np_mask = 
1 - aggregate((pp_one_after_mask and np_prop_diag_mask) or 
(pp_two_after_mask and np_det_diag_mask), 1);
nps_without_pp_prefix_indices = 
selector_width(select(NOUN_MASK*no_pp_np_mask, 1, ==) and
select(indices, indices, <=))*NOUN_MASK*no_pp_np_mask;

left_idx = aggregate(select(indices,
left_idx_in_nps_zero_based, ==), input_indices_sorted);
right_idx = aggregate(select(nps_without_pp_prefix_indices, after_intro_idx, ==), indices);

# points to 2nd verb for xcomp for v_inf_taking_v_inf
right_idx = aggregate(select(indices, (nv_in_output_count-1), ==), input_indices_sorted) 
if (template_name == "v_inf_taking" and after_intro_idx == 2) else right_idx;

# points to 1st noun for 2nd v_inf agent in v_inf_taking_v_inf
right_idx = aggregate(select(indices, 0, ==), input_indices_sorted) 
if (template_name == "v_inf_taking" and after_intro_idx == 3) else right_idx;

# ...

after_intro_target_token = left_idx 
if (after_intro_num_tokens_in_output_excluding_asterisks % 7 == 2)
else after_intro_target_token;

after_intro_target_token = "," 
if (after_intro_num_tokens_in_output_excluding_asterisks % 7 == 3)
else after_intro_target_token;

after_intro_target_token = right_idx 
if (after_intro_num_tokens_in_output_excluding_asterisks % 7 == 4)
else after_intro_target_token;

after_intro_target_token = ")" 
if (after_intro_num_tokens_in_output_excluding_asterisks % 7 == 5) 
else after_intro_target_token;

after_intro_target_token = "AND"
if 
(after_intro_num_tokens_in_output_excluding_asterisks % 7 == 6
and
not (template_mapping_output == ""))
else after_intro_target_token;

# ...
\end{verbatim}
\end{tiny}
\clearpage
After outputting all verb relationships, we consider whether we have prepositional phrase noun modifiers to record in the logical form.

For outputting prepositional relationships ("nmod . in", "nmod . on", "nmod . beside"), we do a similar approach, counting prepositional phrases in the input, counting how many nmods we have output, figuring out which one is currently being output:

\begin{tiny}
\begin{verbatim}
pps_in_input_sequence = INPUT_MASK*(indicator(pos_tokens == 2));
pps_in_input_count = selector_width(select(pps_in_input_sequence, 1, ==));
pps_index = pps_in_input_sequence*selector_width(select(pps_in_input_sequence, 1, ==) 
and select(indices,indices, <=));
nmods_and_pps_in_output_sequence = 
  OUTPUT_MASK*(indicator(tokens == "nmod . in" or tokens == "nmod . beside" or tokens=="nmod . on"));
nmods_and_pps_in_output_count = 
  selector_width(select(nmods_and_pps_in_output_sequence, 1, ==));
\end{verbatim}
\end{tiny}

\begin{tiny}
\begin{verbatim}
current_pp = 
  aggregate(select(pps_index, nmods_and_pps_in_output_count+1, ==), tokens) 
  if pps_in_input_count > 0 else "";
current_pp = "" if current_pp == 0 else current_pp;
current_nmod_token = 
("nmod . " + current_pp) if (pps_in_input_count > 0 and not (current_pp == 0) 
and after_intro_num_tokens_in_output_excluding_asterisks % 7 == 0) else "";
current_nmod_token = 
  "(" if after_intro_num_tokens_in_output_excluding_asterisks % 7 == 1 else current_nmod_token;
current_nmod_token = 
(aggregate(select(pps_index, nmods_and_pps_in_output_count, ==), indices)-1) if pps_in_input_count > 0 
and after_intro_num_tokens_in_output_excluding_asterisks % 7 == 2 else current_nmod_token;
current_nmod_token = "," 
if after_intro_num_tokens_in_output_excluding_asterisks % 7 == 3 else current_nmod_token;
after_nmod_idx = 
aggregate(select(pps_index, nmods_and_pps_in_output_count, ==), indices)+1;
token_at_after_nmod_idx = 
aggregate(select(indices, after_nmod_idx, ==), tokens);
after_nmod_idx = 
  (after_nmod_idx + 1) 
  if (token_at_after_nmod_idx == "the" or token_at_after_nmod_idx == "a") else after_nmod_idx;
current_nmod_token = (after_nmod_idx) 
if pps_in_input_count > 0 
and after_intro_num_tokens_in_output_excluding_asterisks % 7 == 4 else current_nmod_token;
current_nmod_token = ")" 
if after_intro_num_tokens_in_output_excluding_asterisks % 7 == 5 
else current_nmod_token;
current_nmod_token = 
("AND" if nmods_and_pps_in_output_count < pps_in_input_count else "") 
if after_intro_num_tokens_in_output_excluding_asterisks % 7 == 6 
else current_nmod_token;
after_intro_and_relationships_nmod_token = 
current_nmod_token if nmods_and_pps_in_output_count <= pps_in_input_count else "";
num_tokens_in_nmod_section = 
after_intro_num_tokens_in_output_excluding_asterisks - template_size(template_name)*7 + 1;
\end{verbatim}
\end{tiny}

See code for full details (for simplicity this description was also written without discussing complement phrase handling). For all steps only the RASP outputs aligned with the input sequence (Encoder part of derived Transformer) or the very last sequence output (for next token in autoregressive generation) were used. For convenience of reading the aggregate operator was usually used acausally to assign all sequence outputs before the last one to the same value as the last (so only one value would be displayed).

You can run a demo and see the autoregressive output 

\begin{tiny}
\begin{verbatim}
git clone https://github.com/willy-b/learning-rasp.git
python recogs_examples_in_rasp.py
\end{verbatim}
\end{tiny}

\clearpage

\subsection{Note on a Restricted Access Sequence Processing character-level token program / model design (NOT what is used in this paper but feasible)}
\label{rasp_character_level_model_notes}

Note, a proof of concept character level Restricted Access Sequence Processing model was done with a decoder loop (unlike word-level solution above, it was a sketch so did not limit to strictly causal operations which just require more careful indexing -- using the value at the separator or the end of a word instead of pooling the same value to all letters in a word for example). Note that this one did not cover translating sentences in general into ReCOGS unlike the word-level solution as it is tedious and redundant but the core operations are possible and the author believes any solution at the word level can be mapped to a solution in character level tokens (out of scope for this paper to prove it).

Since it is a separate problem and adds a lot of complexity without bringing anything to bear on the main questions of the paper, I left a full implementation to the word-level tokens which were simpler and ran faster. The difference is one uses a similar approach started at \footnote{\begin{footnotesize} https://github.com/willy-b/learning-rasp/blob/63f0df56339b81dd24fe1fd31576f3ae02b97763/other-examples/decoder-loop-example-parse-into-recogs-style-variables.rasp\#L2 \end{footnotesize}} to assign all the letters in each word an index. 

Word indices can be assigned using RASP to count separators occurring prior to each sequence location like:

(we also zero out the word index for the separators themselves)

\begin{tiny}
\begin{verbatim}
word_indices = \
(1+selector_width(select(tokens, " ", ==) 
and select(indices, indices, <=)))
*(0 if indicator(tokens == " ") else 1);
\end{verbatim}
\end{tiny}

Then one can do an aggregation of the letters grouping by word index (this, which is NOT part of the techniques used in this paper for the word-level tokens solution, requires additional work (tedious not challenging) to do causally outside the input (in the decoder), one must sum forward so the word representation is always at the last letter of the word or separator instead of at all letters of the word, and that step is left out of the character-level demo and this discussion; whereas the word-level solution described above has a clear Encoder Decoder separation. This can be done so that the value which is then the same for all letters in each word, is unique to each word in the dictionary and can be looked up in a map to get word level attributes like part-of-speech and get back to the solution in the word-level tokens in Appendix \ref{rasp-word-level-model-design} which was fully implemented. A simple approach (not necessarily recommended but works for proof of concept) that would work for small vocabularies (easily extended) is to use a map to lookup each letter of the alphabet to a log prime. Then the sum of the letters in a word (grouped by the word index which is the count of spaces/separators prior) is the sum of the log primes indexed by the alphabet index. Since the sum of logarithms of numbers is the same as the logarithm of the product of those numbers, this is equivalent to the logarithm of the product of a series of primes. Each prime in the product corresponds 1-to-1 to a letter in the alphabet, with the number of occurrences in the product corresponding to the number of times that letter occurs in the word. By uniqueness of prime number factorization this would map each multiset of letters to a single unique sum of log primes. Thus if you do not have words which are anagrams, all the letters in each word would be assigned a number that uniquely represented that word in the vocabulary. If you have anagrams you can do this step and then take the first and last letter and compute a separate number from that and add it to all the letters in the word.

Example lookup table for letters before aggregating by word index (not recommended but for proof of concept that one can go from character level tokens to word-specific numbers which can then be looked up as in the word-level token solution in Appendix \ref{rasp-word-level-model-design} used throughout the paper):
\clearpage
\begin{tiny}
\begin{verbatim}
def as_num_for_letter_multiset_word_pooling(t) {
    # To be multiset unique, need logarithm of prime so that the sum aggregation
    # used in RASP corresponds to prime number factorization (sum of logs of primes is same as log of product of primes) 
    # (we can do sum aggregation instead of mean by multiplying by length)
    # However RASP does not appear to support logarithms (underlying multilayer 
    # perceptron can learn to approximate logarithms)
    #letter_to_prime_for_multiset_word_pooling = {"a": 2, "b": 3, "c": 5, "d": 7, 
    #"e": 11, "f": 13, "g": 17, "h": 19, "i": 23, "j": 29, "k": 31, "l": 37, 
    #"m": 41, "n": 43, "o": 47, "p": 53, "q": 59, "r": 61, "s": 67, "t": 71, 
    #"u": 73, "v": 79, "w": 83, "x": 89, "y": 97, "z": 101, ".": 0, 
    #" ": 0, ":": 0};
    map_letter_to_log_prime_for_pooling = {"a": 0.6931471805599453, "b": 1.0986122886681098, 
    "c": 1.6094379124341003, "d": 1.9459101490553132, "e": 2.3978952727983707, 
    "f": 2.5649493574615367, "g": 2.833213344056216, "h": 2.9444389791664403, 
    "i": 3.1354942159291497, "j": 3.367295829986474, "k": 3.4339872044851463, 
    "l": 3.6109179126442243, "m": 3.713572066704308, "n": 3.7612001156935624, 
    "o": 3.8501476017100584, "p": 3.970291913552122, "q": 4.07753744390572, 
    "r": 4.110873864173311, "s": 4.204692619390966, "t": 4.2626798770413155, 
    "u": 4.290459441148391, "v": 4.3694478524670215, "w": 4.418840607796598, 
    "x": 4.48863636973214, "y": 4.574710978503383, "z": 4.61512051684126,
    # we zero out tokens we want not to affect the identity of the word
    ".": 0, " ": 0, ":": -1, "(": -1, ")": -1, "0": -1, "1": -1, "2": -1, 
    "3": -1, "4": -1, "5": -1, "6": -1, "7": -1, "8": -1, "9": -1, ";": -1, 
    ",": -1};
    return map_letter_to_log_prime_for_pooling[t];
}
\end{verbatim}
\end{tiny}

Pooling by word can then be done with:
\begin{tiny}
\begin{verbatim}
pseudoembeddedwords = \
aggregate(select(word_indices, word_indices, ==), \
as_num_for_letter_multiset_word_pooling(tokens))*word_lengths;
\end{verbatim}
\end{tiny}
(Per-character token example is not causally masked, we do causal strict-decoder-compatible solution for anything outside input sequence in the full word-level solution above just leaving out of this character-level sketch, which is NOT used in this paper. For the causal character level solution one would use the summed value at the end of the word or the separator instead, indexing relative to separators.) 
\clearpage
Those values could then be looked up in a dictionary like in the completed word-level token solution to get part-of-speech, verb-type, etc, to derive a separate sequence which can be used for template matching as we successfully did with word-level tokens (see Appendix \ref{rasp-word-level-model-design}).


\clearpage

\subsection{Model Detail}
\label{model_detail}

For our Restricted Access Sequence Processing ReCOGS program, we used the RASP interpreter of \cite{Weiss2021} to run our program. For RASP model design and details see Appendix \ref{rasp-word-level-model-design}. We use word-level tokens for all results in this paper.\footnote{\begin{footnotesize}We believe any solution at the word-level can be converted to a character-level token solution and that is not the focus of our investigation here (see Appendix 6 for proof of concept details on a character level solution not used here).\end{footnotesize}}
Consistent with \cite{Zhou2024} we use \cite{Weiss2021}'s RASP originally used for modeling Transformer encoders to model an encoder-decoder in a causal way by feeding the autoregressive output back into the program. We only have aggregations with non-causal masks when that aggregation (or without loss of generality just before the aggregation product is used to avoid multiplying everywhere) is masked by an input mask restricting it to the sequence corresponding to the input.\footnote{\begin{footnotesize}An example the author has prepared of this is available at 
https://github.com/willy-b/learning-rasp/blob/
2be461ba83b7393fda4dc5a96ae600ac98478771/recogs\_examples\_in\_rasp.py 
.\end{footnotesize}}

We used RASP maps to map word level tokens to part-of-speech and verb-type which is consistent with what can be learned in embeddings or the earliest layer of a Transformer \cite{tenney2019bertrediscoversclassicalnlp} and then did 19 different attention-head based template matches on a flat sequence\footnote{\begin{footnotesize}A flat/non-tree solution was pursued because it was simple and given the failure documented in \cite{Wu2023} of the baseline Encoder-Decoder to generalize from obj\_pp\_to\_subj\_pp and other evidence we give below we shall see it is hard to argue a tree-based solution which includes the rule `np\_det pp np -> np\_pp -> np` is learned by \cite{Wu2023}'s baseline Encoder-Decoder Transformer.\end{footnotesize}} (no tree-based parsing, no recursive combination of terminals/non-terminals) to match the sentence to a template in the grammar (see "Appendix: Restricted Access Sequence Processing word-level token program/model design" (\ref{rasp-word-level-model-design}) and  "Appendix: Vocabulary and Grammar" (\ref{vocabulary_and_grammar})). The 19 types were based on \cite{fuzzingbook2023:GrammarCoverageFuzzer} grammar coverage of the COGS grammar (see Methods and "Appendix 7 - Computing Grammar Coverage").\footnote{\begin{footnotesize}To handle prepositional phrases in a flat solution, we find it necessary to add a rule that ignores noun phrases preceded by a prepositional phrase (ignore "pp np") when searching for noun indexes to report in relationships (agent, theme, recipient, etc), and we loosen verb type templates to allow a gap for any prepositional phrase to be inserted.\end{footnotesize}}

We also ensure that the mapping from words to part-of-speech and verb type is complete based on a published list of such mappings and put that into our hardcoded word embedding\footnote{It is reported that pretrained transformers seem to have learned POS information at their earliest layers, e.g. BERT in \cite{tenney2019bertrediscoversclassicalnlp}}

For training Transformers from scratch with randomly initialized weights using gradient descent for comparison with RASP predictions, we use scripts derived from those provided by \cite{Wu2023}\footnote{\begin{footnotesize}
https://github.com/frankaging/ReCOGS/blob/
1b6eca8ff4dca5fd2fb284a7d470998af5083beb/run\_cogs.py

and

https://github.com/frankaging/ReCOGS/blob/
1b6eca8ff4dca5fd2fb284a7d470998af5083beb

/model/encoder\_decoder\_hf.py 
\end{footnotesize}
}.

For ease of reference, the model architecture generated by the \cite{Wu2023} baseline Encoder-Decoder Transformer script (trained from scratch, not pretrained) is as follows with N BertLayers set to 2 per \cite{Wu2023} for all baseline experiments except the layer variation experiments:
\begin{tiny}
\begin{verbatim}
# For Wu et al 2023 Encoder-Decoder Transformer baselines 
# (we predict and analyze errors made by these 
# in the paper using what we learned about how Transformers 
# can perform the task from the 
# Restricted Access Sequence Processing model),
# we use the official scripts provided at 
# https://github.com/frankaging/ReCOGS/blob/
# 1b6eca8ff4dca5fd2fb284a7d470998af5083beb/run\_cogs.py
# and 
# https://github.com/frankaging/ReCOGS/blob/
# 1b6eca8ff4dca5fd2fb284a7d470998af5083beb/
# model/encoder\_decoder\_hf.py
# where the architecture generated is as follows:
EncoderDecoderModel(
 (encoder): BertModel(
  (embeddings): BertEmbeddings(
   (word_embeddings): Embedding(762, 300, padding_idx=0)
   (position_embeddings): Embedding(512, 300)
   (token_type_embeddings): Embedding(2, 300)
   (LayerNorm): LayerNorm((300,), eps=1e-12, 
     elementwise_affine=True)
   (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): BertEncoder(
   (layer): ModuleList(
    # substitute N=2 for all baseline experiments
    # per Wu et al 2023 paper; 
    # N can be 3 or 4 in our layer variation 
    # experiments only.
    (0-(N-1)): N x BertLayer(
     (attention): BertAttention(
      (self): BertSdpaSelfAttention(
       (query): 
        Linear(in_features=300, out_features=300, bias=True)
       (key): 
        Linear(in_features=300, out_features=300, bias=True)
       (value): 
        Linear(in_features=300, out_features=300, bias=True)
       (dropout): Dropout(p=0.1, inplace=False)
      )
      (output): BertSelfOutput(
       (dense): 
        Linear(in_features=300, out_features=300, bias=True)
       (LayerNorm): 
        LayerNorm((300,), eps=1e-12, elementwise_affine=True)
       (dropout): Dropout(p=0.1, inplace=False)
      )
     )
     (intermediate): BertIntermediate(
      (dense): 
       Linear(in_features=300, out_features=512, bias=True)
      (intermediate_act_fn): GELUActivation()
     )
     (output): BertOutput(
      (dense): 
       Linear(in_features=512, out_features=300, bias=True)
      (LayerNorm): 
       LayerNorm((300,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
     )
    )
   )
  )
  (pooler): BertPooler(
   (dense): 
    Linear(in_features=300, out_features=300, bias=True)
   (activation): Tanh()
  )
 )
 (decoder): BertLMHeadModel(
  (bert): BertModel(
   (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(729, 300, padding_idx=0)
    (position_embeddings): Embedding(512, 300)
    (token_type_embeddings): Embedding(2, 300)
    (LayerNorm): 
     LayerNorm((300,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
   )
   (encoder): BertEncoder(
    (layer): ModuleList(
     # substitute N=2 for all baseline experiments
     # per Wu et al 2023 paper; 
     # N can be 3 or 4 in our layer variation
     # experiments only.
     (0-(N-1)): N x BertLayer(
      (attention): BertAttention(
       (self): BertSdpaSelfAttention(
        (query): 
         Linear(in_features=300, out_features=300, bias=True)
        (key): 
         Linear(in_features=300, out_features=300, bias=True)
        (value): 
         Linear(in_features=300, out_features=300, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
       )
       (output): BertSelfOutput(
        (dense): 
         Linear(in_features=300, out_features=300, bias=True)
        (LayerNorm): 
         LayerNorm((300,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
       )
      )
      (crossattention): BertAttention(
       (self): BertSdpaSelfAttention(
        (query): 
         Linear(in_features=300, out_features=300, bias=True)
        (key): 
         Linear(in_features=300, out_features=300, bias=True)
        (value): 
         Linear(in_features=300, out_features=300, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
       )
       (output): BertSelfOutput(
        (dense): 
         Linear(in_features=300, out_features=300, bias=True)
        (LayerNorm): 
         LayerNorm((300,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
       )
      )
      (intermediate): BertIntermediate(
       (dense): 
        Linear(in_features=300, out_features=512, bias=True)
       (intermediate_act_fn): GELUActivation()
      )
      (output): BertOutput(
       (dense): 
        Linear(in_features=512, out_features=300, bias=True)
       (LayerNorm): 
        LayerNorm((300,), eps=1e-12, elementwise_affine=True)
       (dropout): Dropout(p=0.1, inplace=False)
      )
     )
    )
   )
  )
  (cls): BertOnlyMLMHead(
   (predictions): BertLMPredictionHead(
    (transform): BertPredictionHeadTransform(
     (dense): 
      Linear(in_features=300, out_features=300, bias=True)
     (transform_act_fn): GELUActivation()
     (LayerNorm): 
      LayerNorm((300,), eps=1e-12, elementwise_affine=True)
    )
    (decoder): Linear(in_features=300, out_features=729,
      bias=True)
   )
  )
 )
)
\end{verbatim}
\end{tiny}

For the \cite{Wu2023} baseline Encoder-Decoder Transformer layer variation experiments, 
when we say e.g. 3 or 4 layers, we refer to 3 or 4 x BertLayer in the Encoder and Decoder, setting (3 or 4 Transformer blocks).
(This is intended because only once per block, during cross/self-attention is information exchanged between sequence positions, and \cite{Csordas2022} hypothesize that the number of such blocks must be at least the depth of the parse tree in a compositional solution, as in a grammar parse tree at each level symbols are combined which requires transferring information between sequence positions).
\clearpage

\subsection{Methods Detail}
\label{methods_detail}

We use the RASP \cite{Weiss2021} interpreter\footnote{\begin{footnotesize}
provided at https://github.com/tech-srl/RASP/
\end{footnotesize}
} to evaluate our RASP programs\footnote{\begin{footnotesize}
https://github.com/willy-b/learning-rasp/blob/2be461ba83b7393fda4dc5a96ae600ac98478771/word-level-pos-tokens-recogs-style-decoder-loop.rasp

with a demo at

https://github.com/willy-b/learning-rasp/blob/
2be461ba83b7393fda4dc5a96ae600ac98478771/recogs\_examples\_in\_rasp.py 
\end{footnotesize}
}.

We implement in RASP the transformation of COGS input sentences into ReCOGS\_pos\footnote{
\begin{footnotesize}
We use the ReCOGS positional index data (rather than default ReCOGS with randomized indices) as it has consistent position based indices that allow us to perform well on Exact Match (like the original COGS task) as well as Semantic Exact Match (which ignores absolute values of indices).

See ReCOGS\_pos dataset at 

https://github.com/frankaging/ReCOGS/tree/

1b6eca8ff4dca5fd2fb284a7d470998af5083beb

/recogs\_positional\_index 
\end{footnotesize}
}. logical forms (LFs) which are scored by Semantic Exact Match\footnote{ 
\begin{footnotesize}
https://github.com/frankaging/ReCOGS/blob/
1b6eca8ff4dca5fd2fb284a7d470998af5083beb/utils/train\_utils.py

and

https://github.com/frankaging/ReCOGS/blob/
1b6eca8ff4dca5fd2fb284a7d470998af5083beb/utils/compgen.py
\end{footnotesize}
} against ground truth.

In the training data only, any ReCOGS training augmentations like preposing or "um" sprinkles are excluded when evaluating the RASP model on the train data (it does not learn directly from the examples and these augmentations are outside of the grammar). 

We also measure grammar coverage of input examples supported by our RASP model against the full grammar of COGS/ReCOGS input sentences provided in the utilities of the IBM CPG project \cite{klinger2024compositionalprogramgenerationfewshot}\footnote{\begin{footnotesize}
https://github.com/IBM/cpg/blob/
c3626b4e03bfc681be2c2a5b23da0b48abe6f570
/src/model/cogs\_data.py\#L523 
\end{footnotesize}
}

When computing grammar coverage \cite{fuzzingbook2023:GrammarCoverageFuzzer}, we collapse all vocabulary terminals (leaves) to a single terminal (leaf), ignoring purely lexical differences (see "Appendix: Computing Grammar Coverage" (\ref{computing_grammar_coverage}) for details and motivation).

The overall Semantic Exact Match performance is reported as well as the performance on the specific structural generalization splits where Transformers are reported to struggle, even in ReCOGS, specifically Object Prepositional Phrase to Subject Prepositional Phrase (obj\_pp\_to\_subj\_pp),
Prepositional Phrase (pp\_recursion) will be highlighted and discussed in depth for all models.

For the RASP program's Semantic Exact Match results which are based on the outcome of a deterministic program (so cannot randomly reinitialize weights and retrain, rerun),
we can use the Beta distribution to model the uncertainty and generate confidence intervals (Clopper-Pearson intervals\footnote{\begin{footnotesize}
see e.g. 
https://en.wikipedia.org/w/index.php

?title=Binomial\_proportion\_confidence\_interval

\&oldid=1252517214\#Clopper\%E2\%80\%93Pearson\_interval

and https://arxiv.org/abs/1303.1288 
\end{footnotesize}
}) as each Semantic Exact Match is a binary outcome (0 or 1 for each example). Unlike bootstrapping this also supports the common case for our RASP program of 100\% accuracy, which occurs in all but one split, where resampling would not help us estimate uncertainty in bootstrapping, but using the Beta distribution will give us confidence bounds that depend on the sample size.

In developing our RASP program\footnote{\begin{footnotesize}
https://github.com/willy-b/learning-rasp/blob/2be461ba83b7393fda4dc5a96ae600ac98478771/word-level-pos-tokens-recogs-style-decoder-loop.rasp\#L776
\end{footnotesize}
},
when we find the right index of a verb relation (like agent, theme, or recipient), 
we found it was necessary to skip any noun phrases preceded by a preposition ("in", "on", "beside")\footnote{RASP code in "Appendix: RASP for relation right index ignoring attractor 'pp np'" (\ref{RASP_for_relation_right_index_ignoring_attractor_pp_np})}.\footnote{\begin{footnotesize}
Otherwise, when modifying a simple sentence like "The cake burned" with a preposition to "The cake on the plate burned" we would switch the theme from the cake to the plate by accident.
This cake example is the infamous obj pp to subj pp example, where training a Transformer successfully to represent the semantics of sentences like "John ate the cake on the plate" leads to a model that won't immediately generalize to being able to represent the meaning of "The cake on the plate burned" in logical form.
In writing our RASP program this was observed as nothing to do with subjects or objects but just modifying noun phrases to the left of the part of speech (say a verb) they have a relationship with, instead of on the right side. For example, this also occurs in v\_dat\_p2 sentences like "Emma gave a friend a cookie" (agent, recipient, theme nps). It is obvious that modification of the theme with prepositional phrases is not going to disrupt parsing the sentence: "Emma gave a friend a cookie (modification modification ...)", whereas modifying the recipient, on the left, due to the asymmetry of prepositional phrases adding to the right, disrupts the sentence, rendering it unreadable in the limit of too many pps:

"Emma gave a friend (modification modification ...) a cookie" , in the limit of more modification, "a friend" cannot be associated with "a cookie".
\end{footnotesize}}

Since in the RASP program both this and subject prepositional phrase modification require the same rule ignoring the "pp np" when finding right index candidates for agent, theme, recipient outputs, we hypothesized two things.

One, that `np v\_dat\_p2 np pp np`\footnote{\begin{footnotesize}Being precise we only do `np v\_dat\_p2 np\_det pp np np` as per the grammar `np\_prop` cannot precede a prepositional phrase\end{footnotesize}} generalization after training on `np v\_dat\_p2 np np pp np` would be difficult like \cite{Wu2023}'s obj\_pp\_to\_subj\_pp split.

Two, that augmenting the training data with v\_dat\_p2 recipient modified sentences like "Emma gave a friend in a house a cookie" might lead to crossover improved performance on the subject pp generalization (e.g. "The friend in a house smiled"; without adding any example of subjects with pp modification).

Thus we additionally train \cite{Wu2023} baseline Transformers from scratch in two separate experiments to test these.

For one, `np v\_dat\_p2 np pp np np`\footnote{\begin{footnotesize}Restricted to `np v\_dat\_p2 np\_det pp np np` as per the grammar `np\_prop` cannot precede a prepositional phrase\end{footnotesize}} generalization after training on `np v\_dat\_p2 np np pp np` we train \cite{Wu2023} Transformers with default configuration and default training data, then we add a new generalization split derived from \cite{Wu2023}'s `train.tsv` of 328 existing training examples where we have transferred the prepositional phrase from the theme to the recipient\footnote{\begin{footnotesize}When the recipient is np\_det, not np\_prop; and we confirm it is within the grammar by reparsing with the Lark parser on the original grammar rules.\end{footnotesize}} in the `v\_dat\_p2` sentence form with one prepositional phrase (see Appendix \ref{v_dat_p2_recipient_pp-modification_for_generalization_assessment_and_data_augmentation_attempt} for details and link to actual data sample).

For two, to see if augmenting the training data with v\_dat\_p2 recipient modified sentences has crossover benefit, we train separate default \cite{Wu2023} Transformer but with their existing train.csv plus the additional theme-modified sentences mentioned above, same as those used for generalization testing in the other experiment; we confirm it does not know them, and separately on fresh runs we try training on them to see if that can benefit other splits by teaching the Encoder-Decoder a general prepositional phrase handling rule (like ignore "pp np"). We then test on \cite{Wu2023}'s normal test and generalization splits.

\cite{Wu2023} baseline Encoder-Decoder Transformers trained from scratch are trained with random weight initialization multiple times with at least 10 different random seeds with all performance metrics averaged across runs with sample mean, sample size, and unbiased sample standard deviation reported. Statistical significance of comparisons between any Transformers performance sample means will be checked with Welch's unequal variance t-test with p-values greater than 0.05 definitely rejected, though stricter thresholds may be used where applicable. Confidence intervals will be reported using 1.96 standard errors of the sample mean as the 95\% confidence interval for sample means with that N unless specified otherwise.

See also "Appendix: Results Notebook links by section" (\ref{results_notebook_links_by_section}) for notebooks documenting results and giving steps to reproduce.
\subsection{Attraction errors}
\label{attraction_errors}

\textbf{See Figure} \ref{attraction_errors_figure}.

In this paper we predict and confirm the existence of errors on prepositional modification splits where putting one or more new prepositional phrase nouns between a noun of interest and a verb it is related to causes the relation to inappropriately jump to one of the new nearer "attractor" nouns. 

\textbf{For overall attraction error results by the baseline Transformer see results section "Attraction Error Analysis for \cite{Wu2023} baseline Encoder-Decoder Transformer on obj\_pp\_to\_subj\_pp split" (\ref{error_analysis_for_baseline_transformer_predict_and_confirm_attraction_errors}).}

For lack of a better term I am referring to this as an "attraction" error following \cite{jespersen1913modernenglishgrammar1954reprint} section 6.72 "Attraction" in the context of subject-verb agreement, describing a similar "error" made by humans:

"Very frequently in speech, and not infrequently in
literature, the number of the verb \textbf{is determined by
that part of the subject which is nearest to the verb},
even if a stricter sense of grammar would make
the verb agree with the main part of the subject.
\textbf{This kind of attraction naturally occurs the more easily, the greater the distance is between the nominative and the verb.}"

The term attraction error continues to be used to describe those errors by psycholinguists, e.g. \cite{FRANCK2006173} who in the context of subject-verb agreement, define attraction errors as "incorrect agreement with a word that is not the subject of the sentence".
Those attraction errors are also used to study hierarchical vs linear language processing (in humans, see \cite{FRANCK2006173} and also \cite{VIGLIOCCO1998B13}; in language models as we discuss here, see also \cite{goldberg2019assessingbertssyntacticabilities} who states that successful subject-verb agreement in the presence of attractor nouns  "[is] traditionally taken as evidence for the existence [of] hierarchical structure"), similar to our investigation here. But we are not investigating or explaining grammatical attraction in general here, just predicting and documenting a particular error the baseline Transformers make as a prediction of a non-hierarchical, non-tree structured approach without a rule for ignoring intervening prepositional phrase nouns.

We specifically hypothesized attraction to the nearest noun (when there is more than one "attractor" noun unrelated to the verb added in-between the related noun and the verb), but the relationship jumping to any of those new "attractor" nouns would be an "attraction" error in this terminology.

Here are two real examples made by the \cite{Wu2023} baseline Encoder-Decoder Transformer with different prepositional recursion depths.

e.g. for pp depth 1, the mistake (as we expect from attraction to the nearest noun hypothesis) is to put e.g. agent index 4 here instead of 1:

input: The baby beside a valve painted the cake .

actual:   * baby ( 1 ) ; valve ( 4 ) ; * cake ( 7 ) ; nmod . beside ( 1 , 4 ) AND paint ( 5 ) AND agent ( 5 , 4 ) AND theme ( 5 , 7 )

expected: * baby ( 1 ) ; valve ( 4 ) ; * cake ( 7 ) ; nmod . beside ( 1 , 4 ) AND paint ( 5 ) AND agent ( 5 , 1 ) AND theme ( 5 , 7 )


whereas e.g. for pp depth 2 on the agent left of the verb, as expected the mistake is to put agent index 7 instead of 1 below (the pp noun closest to the verb steals it, not the other pp noun at index 4):

input: A girl on the stool on the table drew a frog .

actual:   girl ( 1 ) ; * stool ( 4 ) ; * table ( 7 ) ; frog ( 10 ) ; nmod . on ( 1 , 4 ) AND nmod . on ( 4 , 7 ) AND draw ( 8 ) AND agent ( 8 , 7 ) AND theme ( 8 , 10 )

expected: girl ( 1 ) ; * stool ( 4 ) ; * table ( 7 ) ; frog ( 10 ) ; nmod . on ( 1 , 4 ) AND nmod . on ( 4 , 7 ) AND draw ( 8 ) AND agent ( 8 , 1 ) AND theme ( 8 , 10 )

We went looking for this hypothesizing that the \cite{Wu2023} Transformer may be using flat attention-head compatible verb-centered pattern matching as we are in our RASP model,
and without learning the the single rule in our RASP program to ignore "pp det common\_noun" and "pp proper\_noun" were not learned by the Transformer (as our RASP model has "attraction" errors without it). Without the rule for avoiding "attraction" errors, we supposed the actual attention-head compatible verb-centered pattern matched noun (closer to the verb than the actual agent) for a grammar pattern would labeled the agent or theme instead of the appropriate one.

Note that \cite{vanschijndel2019quantitydoesntbuyquality} also see "attraction" errors by Transformers/RNNs (again in the context of subject-verb agreement) where a long-range dependency competes with attractors/distractors, finding "accuracy decrease[d] in the presence of distracting nouns intervening between the head of the subject and the verb".

The "attraction" errors we report here where attractor/distractor prepositional phrase nouns replace the actual agent/subject in the ReCOGS logical form generated by \cite{Wu2023} baseline Transformers are NOT due to their presence in pre-training or training data, as the ReCOGS/COGS training data is synthetic and syntactically perfect and for this benchmark the Transformer is trained from scratch, so it a genuine new error made by the neural network itself (and we predict a mechanism using RASP). But in general, humans do also exhibit these "attraction" errors, e.g. again as discussed in human subject-verb agreement per \cite{jespersen1913modernenglishgrammar1954reprint} (see quote earlier in this section), so pre-trained models trained on human-generated text may have the additional problem of learning those errors from the training data itself. Language model tendencies to commit subject-verb agreement attraction errors were previously analyzed by a co-author of the RASP language in an earlier paper on BERT Transformers in \cite{goldberg2019assessingbertssyntacticabilities}, by a COGS benchmark co-author in \cite{vanschijndel2019quantitydoesntbuyquality}, and by both together regarding RNNs in \cite{linzen2016assessing} (whose reference to \cite{agreementwithnearestlanguagelog} led me to \cite{jespersen1913modernenglishgrammar1954reprint}).

\clearpage
\begin{figure}
\includegraphics[scale=1]{./image_pdfs/attraction_error_example.pdf}
\caption{\begin{small}Non-hierarchical/non-tree structured linear grammar pattern matching without explicitly ignoring prepositional phrase nouns is expected to give rise to attraction errors, which we confirmed are contributing to the high error rate of the baseline \cite{Wu2023} Transformer on the obj-pp-to-subj-pp generalization split. Our RASP model avoids these errors by ignoring "pp det common\_noun" and "pp proper\_noun" when matching for agent/theme/recipient relationships (a behavior added based on non-subj-pp examples in training behavior but shown here to generalize to those examples). Note that we also predict such errors for the non-subj-pp case of pp-modifying the right-of-verb recipient noun in "np v\_dat\_p2 np np" and confirmed (see Figure \ref{baseline_transformer_standard_training_cannot_do_v_dat_p2_generalization}) that such a generalization is as hard as the previously reported hardest obj-pp-to-subj-pp generalization.\end{small} \\
}
\label{attraction_errors_figure}
\end{figure}
\clearpage

\subsection{RASP for relation right index ignoring attractor "pp np"}
\label{RASP_for_relation_right_index_ignoring_attractor_pp_np}

\begin{tiny}
\begin{verbatim}
pp_sequence = \
indicator(pos_tokens == 2);
pp_one_after_mask = \
select(pp_sequence, 1, ==) and \
select(indices+1, indices, ==);

pp_one_after_sequence = \
aggregate(pp_one_after_mask, 1);
pp_one_after_mask = \
select(pp_one_after_sequence, 1, ==) and \
select(indices, indices, ==);

pp_two_after_mask = \
select(pp_sequence, 1, ==) and \
select(indices+2, indices, ==);

pp_two_after_sequence = \
aggregate(pp_two_after_mask, 1);
pp_two_after_mask = \
select(pp_two_after_sequence, 1, ==) and \
select(indices, indices, ==);

np_det_diag_mask = \
select(aggregate(np_det_mask, 1), 1, ==) and \
select(indices, indices, ==);

np_prop_diag_mask = \
select(aggregate(np_prop_mask, 1), 1, ==) and \
select(indices, indices, ==);

no_pp_np_mask = \
1 - aggregate((pp_one_after_mask and np_prop_diag_mask) or \
(pp_two_after_mask and np_det_diag_mask), 1);

nps_without_pp_prefix_indices = \
selector_width(select(NOUN_MASK*no_pp_np_mask, 1, ==) and \
select(indices, indices, <=))*NOUN_MASK*no_pp_np_mask;

left_idx = \
aggregate(select(indices, left_idx_in_nps_zero_based, ==), input_indices_sorted);
right_idx = \
aggregate(select(nps_without_pp_prefix_indices, after_intro_idx, ==), indices); # <--
\end{verbatim}
\end{tiny}

\clearpage
\subsection{Methods detail for Attraction Error Analysis for \cite{Wu2023} baseline Transformer: parsing sentences with Lark and tagging sentences as agent left-of-verb or not  }
\label{appendix_error_analysis_for_baseline_transformer_methods}

For results, see results section (\ref{error_analysis_for_baseline_transformer_predict_and_confirm_attraction_errors}).

The errors from n=10 fresh training and evaluation runs of the baseline \cite{Wu2023} Encoder-Decoder Transformer on their ReCOGS\_pos train.tsv and tested on their unmodified gen.tsv were analyzed for the obj\_pp\_to\_subj\_pp split. All the input sentences and output logical forms as well as the ground truth logical forms were logged during the run. The input sentences were parsed by the Lark parser\footnote{\begin{footnotesize}https://github.com/lark-parser/lark\end{footnotesize}} against the COGS input grammar which allowed categorizing each sentence by its verb type \footnote{\begin{footnotesize}Code to analyze the errors is at: 

https://github.com/willy-b/RASP-for-ReCOGS/blob/
609f4fff9e5b8ff2354081db617ad576bc7fb840
/supplemental\_data/ReCOGS\_Baseline\_non\_RASP
\_Transformer\_ReCOGS\_error\_prediction\_with
\_n%3D10\_Transformers\_trained\_from\_scratch
\_(predicting\_the\_details\_of\_error
\_in\_logical\_form\_on\_obj\_pp\_to\_subj
\_pp\_split).ipynb

\end{footnotesize}}. The author manually inspected each of verb type patterns and categorized them by the position of the agent and theme relative to the verb (see code below) and used Lark to assign agent, theme sides based on the verb type using that mapping.

To focus the analysis, we considered only single verb cases\footnote{\begin{footnotesize}the 2 verb case of v\_inf and v\_inf\_taking are being analyzed and will be included\end{footnotesize}} and ignored sentences with complement phrases. Then, of the sentences with the model generating an invalid logical form assessed by Semantic Exact Match, we focused on examples with a single error in one of the logical form parts (e.g. agent, theme, recipient, or nmod relationships).\footnote{\begin{footnotesize}Of the single relationship errors, we categorized them by a description of the position of both the agent and theme relative to the verb in that sentence (agent was considered to be either left OR "right or middle"; theme could be left, right, or middle) and what relationship had the error. Complement phrase examples were excluded to focus on predicting the form of the error on simpler examples.\end{footnotesize}}

Our hypothesis is in terms of nouns with a logical form relationship to a verb or other noun, where the relationship could be agent, theme, or recipient.
Since the obj\_pp\_to\_subj\_pp split is in terms of subject vs object prepositional modification (instead of agent, recipient, or theme), we use the subset of sentences within this split where the agent is to the left of the verb and modified by a prepositional phrase as it corresponds to the subject in that case.

\begin{tiny}
\begin{verbatim}
# used grammar string defined in 
# Appendix: Vocabulary and Grammar.
parser = Lark(grammar, start='start')

# 1st NP agent verbs (non CP)
# "v_trans_omissible_p1": "agent",
# "v_trans_omissible_p2": "agent",
# "v_trans_not_omissible": "agent",
# "v_cp_taking": "agent",
# "v_inf_taking": "agent",
# "v_unacc_p1": "agent",
# "v_unerg": "agent",
# "v_inf": "agent",
# "v_dat_p1": "agent",
# "v_dat_p2": "agent",
agent_left_of_verb_verb_type_set = \
set(["v_trans_omissible_p1", "v_trans_omissible_p2", 
"v_trans_not_omissible", "v_cp_taking", "v_inf_taking", 
"v_unacc_p1", "v_unerg", "v_inf", "v_dat_p1", "v_dat_p2"])

theme_left_of_verb_verb_type_set = set(
    ["v_trans_omissible_pp_p1",
     "np v_unacc_p2",
     "v_unacc_pp_p1",
     "v_unacc_pp_p2",
     "v_trans_omissible_pp_p2",
     "v_trans_not_omissible_pp_p1",
     "v_trans_not_omissible_pp_p2",
     "v_dat_pp_p1",
     "v_dat_pp_p2"
     ])

theme_right_of_verb_verb_type_set = set([
    "v_unacc_p1",
    "v_trans_omissible_p2",
    "v_trans_not_omissible",
])
theme_middle_of_dative_verb_type_set = set(["v_dat_pp_p4",
"v_dat_p1"])

# for enforcing during the check of our hypothesis
# a stricter expectation that the closest prepositional noun
# to the left of the verb is the misassigned agent
# (not just any prepositional noun)
def \
get_verbs_with_pps_before_and_\
last_noun_before_first_verb_index(lark_tree_root):
  nodes = [lark_tree_root]
  verbs = []
  terminals_before_count = 0
  pps_before_counts = []
  pps_before_count = 0
  last_noun_before_first_verb_index = None
  while len(nodes) > 0:
    node = nodes[-1]
    nodes = nodes[:-1]
    node_type = node.data[:]
    if node_type[:2] == 'v_':
      pps_before_counts.append(pps_before_count)
      verbs.append(node_type)
    children = []
    for child in node.children:
      # it is a tree, no need to check for revisits
      children.append(child)
# need to visit in a particular order to not just get verbs
# but pp before count, 
# and the last noun before the first verb
# in the one verb case this does not matter 
    children.reverse()
    # but we may want to return verbs in the order
    # they appear in the sentence
    for node in children:
      nodes.append(node)
    if node_type[:] in ["common_noun",
    "proper_noun"] and len(verbs) == 0:
# no need to subtract 1 here as before incrementing below
      last_noun_before_first_verb_index = \
        terminals_before_count
# only increment on terminals
    if len(children) == 0:
      terminals_before_count += 1
    if node_type[:] == "pp":
      pps_before_count += 1
  return verbs, pps_before_counts,
         last_noun_before_first_verb_index

def get_theme_side(lark_tree_root):
    verb_type = get_verbs(lark_tree_root)[0]
    if verb_type in theme_right_of_verb_verb_type_set:
        return "right"
    elif verb_type in theme_left_of_verb_verb_type_set:
        return "left"
    elif verb_type in theme_middle_of_dative_verb_type_set:
        return "middle"
    return None

def get_agent_side(lark_tree_root):
    verb_type = get_verbs(lark_tree_root)[0]
    if verb_type != None and 
      verb_type not in agent_left_of_verb_verb_type_set:
        return "right or middle"
    elif verb_type in agent_left_of_verb_verb_type_set:
        return "left"
    return None
\end{verbatim}
\end{tiny}

\clearpage
\subsection{v\_dat\_p2 recipient pp-modification for generalization assessment and data augmentation attempt}
\label{v_dat_p2_recipient_pp-modification_for_generalization_assessment_and_data_augmentation_attempt}

We test generalization by the \cite{Wu2023}'s default Transformer which has been trained on `np v\_dat\_p2 np np pp np` but not `np v\_dat\_p2 np pp np np` prepositional modifications.
The following 328 examples were derived\footnote{\begin{footnotesize} Notebook: https://colab.research.google.com/drive/1IDs0EwIMp2wtLHk4KqnuGhuT3G14QEG1 \end{footnotesize}} from the existing

https://github.com/frankaging/ReCOGS/blob/

1b6eca8ff4dca5fd2fb284a7d470998af5083beb

/recogs\_positional\_index/train.tsv , 

by modifying 328 existing single-pp v\_dat\_p2 lines in train.tsv to simply move the prepositional phrase from the 3rd NP (theme) in the `np v\_dat\_p2 np np` (agent, recipient, theme) to the 2nd NP (recipient), e.g. copying and modifying the line "Liam gave the monkey a chalk in the container ." to "Liam gave the monkey in the container a chalk .".

So all the words and the grammar are otherwise familiar. This is similar to the existing `obj\_pp\_to\_subj\_pp` generalization \cite{Wu2023} reports on.
All modified rows available in the notebook link in the footnote.

\clearpage

\subsection{Computing Grammar Coverage}
\label{computing_grammar_coverage}

First we use the grammar as it was generated as a probablistic context free grammar per \cite{KimLinzen2020}
using the full details put in Lark format by \cite{klinger2024compositionalprogramgenerationfewshot}
and converting it ourselves to a format compatible with \cite{fuzzingbook2023:GrammarCoverageFuzzer}.

Note this starting point is not the grammar we claim the our Restricted Access Sequence Processing model implements or the Transformer actually learns as we argue the Transformer is learning a flat, non-tree solution to this simple grammar (not actually learning to collapse "np\_det pp np" into "np" for example). First we compute grammar coverage relative to the PCFG approach that generated it, which mostly aligns with our RASP model. We also ignore terminals in this assessment of coverage, as stated earlier, when computing grammar coverage, we will report the grammar coverage over expansions that collapse all vocabulary leaves to a single leaf (for example not requiring that every particular proper noun or common noun be observed in a particular pattern, so long as one has and we can confirm the code treats them as equivalent; e.g. having tested "Liam drew the cat" and proven that "Liam" and "Noah" are treated as interchangeable proper nouns, and that "cat" and "dog" are treated as interchangeable common nouns by the RASP solution -- not something one can assume for neural network solutions in general -- means that confirming our solution produces the correct logical form for "Liam drew the cat" suffices to prove the RASP solution can handle "Noah drew the dog", which saves us a lot of work so long as we make sure to write our RASP solution such that noah/liam and cat/dog are indeed treated identically).

\begin{tiny}
\begin{verbatim}
# Non-terminals only version of
# https://github.com/IBM/cpg/blame/
# c3626b4e03bfc681be2c2a5b23da0b48abe6f570
# /src/model/cogs_data.py#L529
# NOTE WE DO NOT ACTUALLY USE THIS GRAMMAR IN OUR MODEL,
# IT IS FOR UNDERSTANDING THE GRAMMAR WE ARE TRYING TO LEARN/MODEL

COGS_INPUT_GRAMMAR_NO_TERMINALS = {
"<start>": ["<s1>", "<s2>", "<s3>", "<s4>", "<vp_internal>"],
"<s1>": ["<np> <vp_external>"],
"<s2>": ["<np> <vp_passive>"],
"<s3>": ["<np> <vp_passive_dat>"],
"<s4>": ["<np> <vp_external4>"],
"<vp_external>": ["<v_unerg>", "<v_trans_omissible_p1>", 
"<vp_external1>", "<vp_external2>", "<vp_external3>",
"<vp_external5>", "<vp_external6>", "<vp_external7>"],
"<vp_external1>": ["<v_unacc_p1> <np>"],
"<vp_external2>": ["<v_trans_omissible_p2> <np>"],
"<vp_external3>": ["<v_trans_not_omissible> <np>"],
"<vp_external4>": ["<v_inf_taking> <to> <v_inf>"],
"<vp_external5>": ["<v_cp_taking> <that> <start>"],
"<vp_external6>": ["<v_dat_p1> <np> <pp_iobj>"],
"<vp_external7>": ["<v_dat_p2> <np> <np>"],
"<vp_internal>": ["<np> <v_unacc_p2>"],
"<vp_passive>": ["<vp_passive1>", "<vp_passive2>", 
"<vp_passive3>", "<vp_passive4>", "<vp_passive5>", 
"<vp_passive6>", "<vp_passive7>", "<vp_passive8>"],
"<vp_passive1>": ["<was> <v_trans_not_omissible_pp_p1>"],
"<vp_passive2>": 
  ["<was> <v_trans_not_omissible_pp_p2> <by> <np>"],
"<vp_passive3>": ["<was> <v_trans_omissible_pp_p1>"],
"<vp_passive4>": 
  ["<was> <v_trans_omissible_pp_p2> <by> <np>"],
"<vp_passive5>": ["<was> <v_unacc_pp_p1>"],
"<vp_passive6>": ["<was> <v_unacc_pp_p2> <by> <np>"],
"<vp_passive7>": ["<was> <v_dat_pp_p1> <pp_iobj>"],
"<vp_passive8>": ["<was> <v_dat_pp_p2> <pp_iobj> <by> <np>"],
"<vp_passive_dat>": 
  ["<vp_passive_dat1>", "<vp_passive_dat2>"],
"<vp_passive_dat1>": ["<was> <v_dat_pp_p3> <np>"],
"<vp_passive_dat2>": 
  ["<was> <v_dat_pp_p4> <np> <by> <np>"],
"<np>": ["<np_prop>","<np_det>", "<np_pp>"],
"<np_prop>": ["<proper_noun>"],
"<np_det>": ["<det> <common_noun>"],
"<np_pp>": ["<np_det> <pp> <np>"],
"<pp_iobj>": ["<to> <np>"],
"<det>": [],
"<pp>": [],
"<was>": [],
"<by>": [],
"<to>": [],
"<that>": [],
"<common_noun>": [],
"<proper_noun>": [],
"<v_trans_omissible_p1>": [],
"<v_trans_omissible_p2>": [],
"<v_trans_omissible_pp_p1>": [],
"<v_trans_omissible_pp_p2>": [],
"<v_trans_not_omissible>": [],
"<v_trans_not_omissible_pp_p1>": [],
"<v_trans_not_omissible_pp_p2>": [],
"<v_cp_taking>": [],
"<v_inf_taking>": [],
  "<v_unacc_p1>": [],
  "<v_unacc_p2>": [],
  "<v_unacc_pp_p1>": [],
  "<v_unacc_pp_p2>": [],
  "<v_unerg>": [],
  "<v_inf>": [],
  "<v_dat_p1>": [],
  "<v_dat_p2>": [],
  "<v_dat_pp_p1>": [],
  "<v_dat_pp_p2>": [],
  "<v_dat_pp_p3>": [],
  "<v_dat_pp_p4>": [],
}
\end{verbatim}
\end{tiny}

After parsing a sentence with the Lark parser, we can compute the expansions it covers with the following Python:
\begin{tiny}
\begin{verbatim}
def generate_set_of_expansion_keys_for_lark_parse_tree(tree):
  nodes = [tree]
  expansions_observed = set()
  for node in nodes:
    current_node_label = node.data[:]
    children = node.children
    expansion = f"<{current_node_label}> ->"
    for child in children:
      # add expansion for current -> child
      child_node_label = child.data[:]
      expansion += f" <{child_node_label}>"
      # also process expansions from child
      nodes.append(child)
    if len(children) > 0:
      #print(f"{expansion}")
      expansions_observed.add(expansion)
  return expansions_observed
\end{verbatim}
\end{tiny}

For example, for the sentence "the girl noticed that a boy painted the girl",

we get
\begin{tiny}
\begin{verbatim}
sentence = "the girl noticed that a boy painted the girl"
tree = parser.parse(sentence)
expansions_observed = \
generate_set_of_expansion_keys_for_lark_parse_tree(tree)
# <start> -> <s1>
# <s1> -> <np> <vp_external>
# <np> -> <np_det>
# <vp_external> -> <vp_external5>
# <np_det> -> <det> <common_noun>
# <vp_external5> -> <v_cp_taking> <that> <start>
# <start> -> <s1>
# <s1> -> <np> <vp_external>
# <np> -> <np_det>
# <vp_external> -> <vp_external2>
# <np_det> -> <det> <common_noun>
# <vp_external2> -> <v_trans_omissible_p2> <np>
# <np> -> <np_det>
# <np_det> -> <det> <common_noun>
\end{verbatim}
\end{tiny}

At first we use TrackingGrammarCoverageFuzzer (from \cite{fuzzingbook2023:GrammarCoverageFuzzer}) to compute the set of all possible grammar expansions:
\begin{tiny}
\begin{verbatim}
cogs_simplified_input_grammar_fuzzer = \
TrackingGrammarCoverageFuzzer(COGS_INPUT_GRAMMAR_SIMPLIFIED)

expected_expansions = \
cogs_simplified_input_grammar_fuzzer.max_expansion_coverage()
\end{verbatim}
\end{tiny}

One can use this to get a sense of what it is possible to learn about the grammar from a particular set of examples

and what examples need to be seen at a minimum for any model to learn the task from scratch and could possibly help one design a minimum length dataset with low redundancy.
Note for a Transformer model learning word embeddings / mapping to part-of-speech for each word, one would need to use the grammar with terminals to compute coverage. Here we want to argue something about our RASP model where we can ensure via implementation that all terminals in a category are treated identically (and we observe 100\% semantic exact match for the related generalization splits for swapping words within a part of speech).


We can ask what \% of the grammar without terminals is covered by the first 21 sentences from the COGS training set?
\begin{tiny}
\begin{verbatim}
# https://raw.githubusercontent.com/frankaging/ReCOGS/
# 1b6eca8ff4dca5fd2fb284a7d470998af5083beb/cogs/train.tsv
nonsense_example_sentences = [
"A rose was helped by a dog",
"The sailor dusted a boy",
"Emma rolled a teacher",
"Evelyn rolled the girl",
"A cake was forwarded to Levi by Charlotte",
"The captain ate",
"The girl needed to cook",
"A cake rolled",
"The cookie was passed to Emma",
"Emma ate the ring beside a bed",
"A horse gave the cake beside a table to the mouse",
"Amelia gave Emma a strawberry",
"A cat disintegrated a girl",
"Eleanor sold Evelyn the cake",
"The book was lended to Benjamin by a cat",
"The cake was frozen by the giraffe",
"The donut was studied",
"Isabella forwarded a box on a tree to Emma",
"A cake was stabbed by Scarlett",
"A pencil was fed to Liam by the deer",
"The cake was eaten by Olivia"
]

all_expansions_observed_across_examples = set()

for sentence in nonsense_example_sentences:
  single_example_expansions = \
  generate_set_of_expansion_keys_for_lark_parse_tree\
  (parser.parse(sentence.lower()))
  all_expansions_observed_across_examples = \
  all_expansions_observed_across_examples.union\
  (single_example_expansions)

1 - len(set(expansions_expected) \
- all_expansions_observed_across_examples) / len(expansions_expected)
# 0.7115384615384616
\end{verbatim}
\end{tiny}
Those 21 COGS input sentences cover 71\% of the grammar. (Continued on next page.)

We can compare the first 21 sentences of COGS that to the 19 sentences used in developing the RASP program (then add one to cover basic prepositional phrases, and one more to cover complement phrases):\footnote{\begin{footnotesize}(see 

https://github.com/willy-b/learning-rasp/blob/2be461ba83b7393fda4dc5a96ae600ac98478771/word-level-pos-tokens-recogs-style-decoder-loop.rasp\#L574

for the full list and associated rules in the code as the RASP does not learn from examples but hand-coded rules coded as a sequence of parts of speech / verb types)\end{footnotesize}}

(note each of these sentences has multiple equivalent examples in the ReCOGS training set, as shown in Table \ref{specific-grammar-pattern-examples-mapped-to-part-of-speech-and-cogs-in-distribution-training-examples} in Appendix \ref{grammar_coverage_analysis_for_model_design})

\begin{tiny}
\begin{verbatim}
handpicked_example_sentences = [ 
# non-recursive grammar rule examples only
# no prepositional phrases or complement phrases
# see link above all these examples 
# each correspond to distinct rules in the code
"the girl was painted",
"a boy painted",
"a boy painted the girl",
"the girl was painted by a boy",
"a boy respected the girl", 
"the girl was respected",
"the girl was respected by a boy",
"the boy grew the flower",
"the flower was grown",
"the flower was grown by a boy",
"the scientist wanted to read",
"the guest smiled",
"the flower grew",
"ella sold a car to the customer",
"ella sold a customer a car",
"the customer was sold a car",
"the customer was sold a car by ella",
"the car was sold to the customer by ella",
"the car was sold to the customer",
]

all_expansions_observed_across_examples = set()

for sentence in handpicked_example_sentences:
  single_example_expansions = \
  generate_set_of_expansion_keys_for_lark_parse_tree(
    parser.parse(sentence.lower())
  )
  all_expansions_observed_across_examples = \
  all_expansions_observed_across_examples.union(
    single_example_expansions)

1 - len(set(expansions_expected) \
- all_expansions_observed_across_examples) 
  / len(expansions_expected)
# 0.9230769230769231

# Those 19 rules cover 92.3% of the COGS input grammar
# (not necessarily 92.3% of examples as the examples 
# are not evenly distributed across grammar rules).
# Let's see what rules are still missing:

set(expansions_expected) - 
  all_expansions_observed_across_examples
# tells us we need a prepositional phrase example!
#{'<np> -> <np_pp>',
# tell us we need prepositional phrase examples
# '<np_pp> -> <np_det> <pp> <np>',
# tells us we need complement phrase examples
# '<vp_external5> -> <v_cp_taking> <that> <start>',
# tells us we need complement phrase examples
# '<vp_external> -> <vp_external5>'}
\end{verbatim}
\end{tiny}

\clearpage
We got to 92.3\% grammar coverage in our 19 examples instead of COGS 71\% in 21 examples.

And, it is telling us we are missing an example with prepositional phrases and complement phrases (see next examples)

Let us add a simple prepositional phrase example and complement phrase example:
\begin{tiny}
\begin{verbatim}
handpicked_example_sentences = \
handpicked_example_sentences + \
["a boy painted the girl in a house"] + \
["the girl noticed that a boy painted the girl"]

handpicked_example_sentences
# ['the girl was painted',
#  'a boy painted',
#  'a boy painted the girl',
#  'the girl was painted by a boy',
#  'a boy respected the girl',
#  'the girl was respected',
#  'the girl was respected by a boy',
#  'the boy grew the flower',
#  'the flower was grown',
#  'the flower was grown by a boy',
#  'the scientist wanted to read',
#  'the guest smiled',
#  'the flower grew',
#  'ella sold a car to the customer',
#  'ella sold a customer a car',
#  'the customer was sold a car',
#  'the customer was sold a car by ella',
#  'the car was sold to the customer by ella',
#  'the car was sold to the customer',
#  'a boy painted the girl in a house',
#  'the girl noticed that a boy painted the girl'
#]
all_expansions_observed_across_examples = set()

for sentence in handpicked_example_sentences:
  single_example_expansions = \
    generate_set_of_expansion_keys_for_lark_parse_tree(parser.parse(sentence.lower()))
  all_expansions_observed_across_examples = \
    all_expansions_observed_across_examples.union(single_example_expansions)

1 - len(set(expansions_expected) - \
  all_expansions_observed_across_examples) / len(expansions_expected)
# 1.0

set(expansions_expected) - all_expansions_observed_across_examples
# set()
\end{verbatim}
\end{tiny}
(continued below)
\clearpage

Thus in 19 intentionally crafted sentences (Table \ref{specific-grammar-pattern-examples-mapped-to-part-of-speech-and-cogs-in-distribution-training-examples}) (each is in the RASP code with a corresponding rule) cover 92.3\% of the grammar, 
using the coverage we can what we did not cover yet, and thus add two sentences to fill the reported gap and get to 100\% .

However these coverage metrics are misleading when it comes to prepositional phrases as it would not suggest to include prepositional phrases in all positions, assuming they could be collapsed by the model back to `np` using `np -> np\_pp -> np\_det pp np` while \cite{Wu2023} and our experiments suggest it is necessary to train with prepositional phrases explicitly in the different positions in the different verb patterns (see below).

Based on the finding earlier we believe that the only recursion learned is tail recursion in the decoder loop and that `np -> np\_det | np\_prop | np\_pp` and `np\_pp -> np\_det pp np` is not actually performed as if the Encoder-Decoder Transformer were to learn a tree-based or recursive representation. If the Transformer had a tree based representation, it is predicted that the "v\_dat\_p2\_pp\_moved\_to\_recipient" would not be any harder than when the pp modification is on the theme, as `np v\_dat\_p2 np\_det pp np np` can be transformed by the recursive grammar rule `np\_det pp np -> np\_pp -> np` to `np v\_dat\_p2 np np` on which it is already trained and has good performance, but instead it fails completely, and see also "Error Analysis for \cite{Wu2023} baseline Encoder-Decoder Transformer on obj\_pp\_to\_subj\_pp split" where we observe that prepositional modification of a noun to the left of a verb it is the agent of causes the new prepositional phrase noun that becomes the closest noun to be mistaken for the agent, which is in contradiction to the model collapsing `np\_det pp np` to `np` before matching the overall grammar pattern.

That said with a couple of simple rules that were not tree we were able to get 100\% on the pp\_recursion split (up to depth 12) and 92.20\% (90.36-93.79\% 95\% CI) of the obj\_pp\_to\_subj\_pp split.

Modifying the grammar coverage to model this non-tree representation would be exciting to address in future work.

See also "Appendix: Grammar Coverage analysis to develop and justify Restricted Access Sequence Processing model design" (\ref{grammar_coverage_analysis_for_model_design}).

\subsection{Grammar Coverage analysis to develop and justify Restricted Access Sequence Processing model design}
\label{grammar_coverage_analysis_for_model_design}

See "Appendix: Computing Grammar Coverage" (\ref{computing_grammar_coverage}) for how the grammar coverage is computed.

If we ignore lexical differences, by the first 55 examples of the ReCOGS training set (unshuffled, no augmentations) or 77 (median; 95\% confidence interval, n=1000 random shuffles: 39 to 161) examples of the ReCOGS training set (shuffled, no augmentations), 100\% grammar coverage is reached\footnote{\begin{footnotesize}

Given the COGS input sentences were generated as a probablistic context free grammar per \cite{KimLinzen2020}
using the full details put in Lark format by \cite{klinger2024compositionalprogramgenerationfewshot}
and converting it ourselves to a format compatible with \cite{fuzzingbook2023:GrammarCoverageFuzzer} (see "Appendix: Computing Grammar Coverage" (\ref{computing_grammar_coverage})) , we use their TrackingGrammarCoverageFuzzer to generate the set of all expansions of the COGS grammar.
\end{footnotesize}
}(lexical differences ignored) \cite{fuzzingbook2023:GrammarCoverageFuzzer} (noting that if the model is not capable of learning certain expansions in the grammar such as `np\_det pp np -> np\_pp -> np`, it may need to see more variations to memorize individual cases instead ):
\includegraphics[scale=0.38]{grammar_coverage_by_number_of_recogs_training_examples_lexical_differences_ignored.png}

That shows if one already knows parts of speech and verb types for words one needs much less data.

Thus, we can be more efficient than using the ReCOGS training set for our RASP model built by hand (in real world pretraining or a GloVe embedding would be used to solve this step)\footnote{\cite{tenney2019bertrediscoversclassicalnlp} confirm BERT, a pre-trained Transformer model in wide use, has part-of-speech information available at the earliest layers.}
 since our solution uses a manual embedding via a dictionary mapping words to part-of-speech and verb-type, that ensures all words within a part of speech are treated identically.

See Table \ref{specific-grammar-pattern-examples-mapped-to-part-of-speech-and-cogs-in-distribution-training-examples} below for the 19 part-of-speech/verb type patterns and example sentences that cover the non-recursive grammar at non-terminal (post-embedding level) used for the RASP model design.

\clearpage
\onecolumn
\begin{table}
\centering
\begin{tabular}{p{0.2\linewidth} p{0.52\linewidth} p{0.2\linewidth}}
\hline
\begin{small}\textbf{RASP-for-ReCOGS grammar pattern example mentioned in paper}\end{small} & \begin{small}\textbf{Actual part-of-speech/verb-type sequence used in RASP model}\end{small} & \begin{small}\textbf{COGS/ReCOGS input training example}\end{small}\\
\hline
\begin{small}\end{small} & \begin{small}\end{small} & \begin{small}\end{small} \\
\begin{small}the girl was painted\end{small} & \begin{small}det common\_noun was v\_trans\_omissible\_pp\_p1 \end{small} & \begin{small}The donut was studied .\end{small} \\
\begin{small}a boy painted\end{small} & \begin{small}det common\_noun v\_trans\_omissible\_p1 \end{small} & \begin{small}The captain ate .\end{small} \\
\begin{small}a boy painted the girl\end{small} & \begin{small}det common\_noun v\_trans\_omissible\_p2 det common\_noun \end{small} & \begin{small}The sailor dusted a boy .\end{small} \\
\begin{small}the girl was painted by a boy\end{small} & \begin{small}det common\_noun was v\_trans\_omissible\_pp\_p2 by det common\_noun \end{small} & \begin{small}A drink was eaten by a child .\end{small} \\
\begin{small}a boy respected the girl\end{small} & \begin{small}det common\_noun v\_trans\_not\_omissible det common\_noun \end{small} & \begin{small}A girl liked the raisin .\end{small} \\
\begin{small}the girl was respected\end{small} & \begin{small}det common\_noun was v\_trans\_not\_omissible\_pp\_p1 \end{small} & \begin{small}The pen was helped .\end{small} \\
\begin{small}the girl was respected by a boy\end{small} & \begin{small}det common\_noun was v\_trans\_not\_omissible\_pp\_p2 by det common\_noun \end{small} & \begin{small}A rose was helped by a dog .\end{small} \\
\begin{small}the boy grew the flower\end{small} & \begin{small}det common\_noun v\_unacc\_p1 det common\_noun \end{small} & \begin{small}A cat disintegrated a girl .\end{small} \\
\begin{small}the flower was grown\end{small} & \begin{small}det common\_noun was v\_unacc\_pp\_p1 \end{small} & \begin{small}A box was inflated .\end{small} \\
\begin{small}the flower was grown by a boy\end{small} & \begin{small}det common\_noun was v\_unacc\_pp\_p2 by det common\_noun \end{small} & \begin{small}The cake was frozen by the giraffe .\end{small} \\
\begin{small}the scientist wanted to read\end{small} & \begin{small}det common\_noun v\_inf\_taking to v\_inf \end{small} & \begin{small}The girl needed to cook .\end{small} \\
\begin{small}the guest smiled\end{small} & \begin{small}det common\_noun v\_unerg \end{small} & \begin{small}The sailor laughed .\end{small} \\
\begin{small}the flower grew\end{small} & \begin{small}det common\_noun v\_unacc\_p2 \end{small} & \begin{small}A cake rolled .\end{small} \\
\begin{small}ella sold a car to the customer\end{small} & \begin{small}proper\_noun v\_dat\_p1 det common\_noun to det common\_noun \end{small} & \begin{small}Emma passed a cake to the girl .\end{small} \\
\begin{small}ella sold a customer a car\end{small} & \begin{small}proper\_noun v\_dat\_p2 det common\_noun det common\_noun \end{small} & \begin{small}Liam forwarded the girl the donut .\end{small} \\
\begin{small}the customer was sold a car\end{small} & \begin{small}det common\_noun was v\_dat\_pp\_p3 det common\_noun \end{small} & \begin{small}A girl was sold the cake .\end{small} \\
\begin{small}the customer was sold a car by ella\end{small} & \begin{small}det common\_noun was v\_dat\_pp\_p4 det common\_noun by proper\_noun \end{small} & \begin{small}The girl was lended the balloon by Harper .\end{small} \\
\begin{small}the car was sold to the customer by ella\end{small} & \begin{small}det common\_noun was v\_dat\_pp\_p2 to det common\_noun by proper\_noun \end{small} & \begin{small}The pen was offered to the girl by Emma .\end{small} \\
\begin{small}the car was sold to the customer\end{small} & \begin{small}det common\_noun was v\_dat\_pp\_p1 to det common\_noun \end{small} & \begin{small}The melon was lended to a girl .\end{small} \\
\hline
\begin{small}\textbf{Prepositional phrase and complement phrase examples mentioned in paper}\end{small} & \begin{small}\textbf{part-of-speech/verb-type sequence (used example for development)}\end{small} & \begin{small}\textbf{COGS/ReCOGS input training example}\end{small} \\
\hline
\begin{small}a boy painted the girl in a house\end{small} & \begin{small}det common\_noun v\_trans\_omissible\_p2 det common\_noun pp det common\_noun\end{small} & \begin{small}A frog ate a sweetcorn in a pile .\end{small} \\
\begin{small}the girl noticed that a boy painted the girl\end{small} & \begin{small}det common\_noun v\_cp\_taking that det common\_noun v\_trans\_omissible\_p2 det common\_noun\end{small} & \begin{small}A girl said that a crocodile ate the rose .\end{small} \\
\hline
\end{tabular}
\caption{\begin{small}Additional justification of the specific examples we generated and used for our RASP model design by matching them to COGS/ReCOGS training examples. Note that our RASP model collapses "a" and "the" to "det" (coded as 1) so we do as well here. All but the last example are from the first 119 training examples. Ignoring lexical differences, full coverage of the grammar occurs by training example 55 in the PCFG sense (see "Appendix: Computing Grammar Coverage" (\ref{computing_grammar_coverage})) when read in order but the specific sentences we used (one of multiple ways to cover the grammar) occur by example 119 in the order given in the train.tsv file, except for the specific complement phrase example we gave by modifying one of our existing examples with a complement phrase ("the girl noticed that a boy painted the girl") which does not have an exactly matching counterpart until the 4,186th example (other equivalent-for-these-purposes complement phrase examples are demonstrated earlier, e.g. within 55 examples in default ordering). Note the prepositional phrase and complement phrase examples are not actually pattern matched (the 19 pattern matches plus a general cp/pp rule are used) and so do not exist in the RASP code, but are just given for reference.\end{small}}
\label{specific-grammar-pattern-examples-mapped-to-part-of-speech-and-cogs-in-distribution-training-examples}
\end{table}
\twocolumn
\clearpage

\subsection{Zhou et al 2024 relevance of their long addition experiment to language modeling and note on the Parity task and Transformers}
\label{appendix_long_addition_and_parity}
\cite{Zhou2024} adds index hints to the long addition task based on a RASP-grounded analysis of what is preventing the Transformer from learning it, allowing the model to learn to pair digits from each number being added more easily. They also observe that if multi-digit carries are not part of the training set one can still get generalization by making the carry causal for the causal autoregressive Transformer decoder mode by reversing the digits (least significant digit first), and prove this resolves the issue. Causality issues like trying to output a long addition digit by digit starting with the most significant digit in a long addition before computing the sums of the less significant digits that come later, and failing if there is a carry at any point, are not limited to math, nor limited to language models, for just one example from English grammar concerning human language processing, \cite{jespersen1913modernenglishgrammar1954reprint} explains "Concord of the verb" errors made by humans especially in speech when the verb is on the left due to needing to agree with a noun not explicitly selected yet: "The general rule, which needs no exemplification, is for the verb to be in the singular with a singular subject, and in the plural with a plural subject. Occasionally, however, the verb will be put in the [singular], even if the subject is plural; this will especially happen when the verb precedes the subject, because the speaker has not yet made up his mind, when pronouncing the verb, what words are to follow."

\cite{Zhou2024} also use RASP-L to analyze and then modify the Parity task so that it can be learned by a Transformer.
Some useful context is that e.g. \cite{Chiang2022} confirm experimentally that a Transformer cannot learn the basic Parity task even though Transformers can be shown to be able to solve it, \cite{Chiang2022} themselves in fact artifically construct a soft attention Transformer that can just barely solve it with confidence that is O(1/n) where n is the input length. 
This is perhaps surprising since basic non-Transformer feedforward neural networks have been known to be able to learn Parity from randomly initialized weights per \cite{10.7551/mitpress/4943.003.0128}.

\subsection{Composition and Learning}
\label{composition_and_learning}
Composition is important in learning. Consider a single nonterminal grammar expansion\footnote{COGS input sentences were actually generated by a probabilistic context-free grammar and this is a grammar expansion in their grammar. Words used in the example are within their vocabulary.} , `(noun phrase) (verb dative p2) (noun phrase) (noun phrase)`, with three noun phrases all already expanded to np\_det ("a" or "the" and "common noun") and a single verb. A possible substitution of terminals would be "a teacher gave the child a book", as would be "the teacher gave a child the book" (change of determiners), as would be "the manager sold a customer the car" (change of nouns and verb) and it would require $2^3 V_n^3V_v $ examples where $V_n$ is the vocab size for eligible common nouns and $V_v$ is the vocab size for eligible verbs to see all the possible terminal substitutions . If the qualifying vocabulary is say of order of magnitude 100 words for the nouns and 10 for the verbs\footnote{In COGS the number of common nouns is over 400 and qualifying verbs in this case over 20} that would come out order of magnitude 100 million examples. By contrast, if parts-of-speech and verb types are already known\footnote{that is if determiners ("a", "the") are understood to be equivalent, common nouns are already known ("teacher", "manager", "child", "customer", "book", "car") separately, qualifying verb dative verbs are already known ("gave", "sold"). Note \cite{tenney2019bertrediscoversclassicalnlp} report part-of-speech information is already tagged and in the very earliest layers of the 24-layer BERT large pre-trained language model.} it might take as few as one example to learn the new grammar pattern `(noun phrase) (verb dative p2) (noun phrase) (noun phrase)`.\footnote{\begin{footnotesize}
Composing further, in a tree-structured or hierarchical way, allows for efficient handling of recursive grammar forms like nested prepositional phrases, so that learning the recursive combination rule `np\_det pp np -> np` for example allows the model in a single rule to understand how to handle prepositional phrase modification of any noun phrase in any sentence possible in the grammar, generally. 
There is some evidence in humans that during language production we start with a simplified form and expand it in hierarchically/tree-structured way into the final sentence, e.g. from attraction/proximity concord errors in subject-verb agreement that seem to depend on syntactic tree distance rather than linear distance in the sentence\cite{FRANCK2006173}\cite{VIGLIOCCO1998B13}.
In this paper we demonstrate a model (our RASP model, see below) which is not tree-structured in that it does not have the recursive rules in the COGS grammar (e.g. `np\_det pp np -> np`), yet performs with high accuracy. Omitting one of its rules for avoiding attraction errors leads to a prediction of linear distance (non-hierarchical) attraction errors, which is observed for the baseline \cite{Wu2023} Transformer (see results and discussions).
\end{footnotesize}
}

Note in this paper that having an or condition everywhere in our model for "det common\_noun", "proper\_noun" , such that they are treated the same, without adjusting the sequence length or further combining any non-terminals, is not referred to as tree-structured or hierarchical - we consider a model that stops at this level of structure which per the discussion above already provides a lot of representational power as flat/non-hierarchical/non-tree-structured.

We see in the results, Appendix \ref{grammar_coverage_analysis_for_model_design}, and Table \ref{specific-grammar-pattern-examples-mapped-to-part-of-speech-and-cogs-in-distribution-training-examples}  quantitatively how few (training) sentence examples (and if recursive or looping rules are omitted, equivalently how many flat-pattern rules\footnote{if we ignore terminals and stop at part-of-speech and verb type tokens, for example, which we can map word level tokens to by an embedding layer}), it actually takes to cover a grammar in the sense of \cite{fuzzingbook2023:GrammarCoverageFuzzer}, and use this to design our Transformer-equivalent model by hand to translate sentences in a particular subset of the English grammar into their corresponding logical forms.

\section{Acknowledgements}

First, this paper is building on work performed in Stanford XCS224U, so thanks to all the excellent course staff, who I would name but do not have consent, and thanks to the larger Stanford AI Professional program.

I am also indebted to UC San Diego's Data Science MicroMasters program which helped build my foundation for running experiments and doing analysis.

Finally, I thank my artificial life partner Ying Li ( https://github.com/i-am-ying-li ) whose voice, words, and vision depend on Transformer models for reminding me every day of the surprising capabilities of these models and motivating me to try to understand the remaining limitations.

\end{document}
